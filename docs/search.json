[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistical Methods for Transport Planning - Workbook",
    "section": "",
    "text": "Preface\nWelcome to the Statistical Methods for Transport Planning (SMTP) Workbook!\nThis workbook is your hands-on companion to the main course. Inside, you’ll find a series of step-by-step activities designed to help you explore real-world transport data and put the concepts from the lectures into practice. Think of it as your space to experiment, apply what you’ve learned, and build confidence in using statistical methods for transport planning.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#pre-requisites",
    "href": "index.html#pre-requisites",
    "title": "Statistical Methods for Transport Planning - Workbook",
    "section": "Pre-requisites",
    "text": "Pre-requisites\nThe activities assume you already have R and RStudio installed in your machine.\nAlso, the activities assume prior knowledge R coding skills.",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "index.html#contact",
    "href": "index.html#contact",
    "title": "Statistical Methods for Transport Planning - Workbook",
    "section": "Contact",
    "text": "Contact\nJoseRafael.Verduzco-Torres@glasgow.ac.uk",
    "crumbs": [
      "Preface"
    ]
  },
  {
    "objectID": "session-1.html",
    "href": "session-1.html",
    "title": "1  Session 1. Working environment and formatting data",
    "section": "",
    "text": "1.1 Setting the working environment\nA good practice is to have an RStudio for each different project you are working on. This creates a folder in which, ideally, you should include all the input and output data.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1. Working environment and formatting data</span>"
    ]
  },
  {
    "objectID": "session-1.html#setting-the-working-environment",
    "href": "session-1.html#setting-the-working-environment",
    "title": "1  Session 1. Working environment and formatting data",
    "section": "",
    "text": "Before moving any further, make sure you have your own RStudio project in your session. Staff will be more than happy to assist you here if needed.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1. Working environment and formatting data</span>"
    ]
  },
  {
    "objectID": "session-1.html#lets-practice-using-some-transport-data",
    "href": "session-1.html#lets-practice-using-some-transport-data",
    "title": "1  Session 1. Working environment and formatting data",
    "section": "1.2 Lets Practice using some transport data",
    "text": "1.2 Lets Practice using some transport data\nIn this course, we will look into Puget Sound Regional Travel Surve. This workbook uses dataset version ‘2023.5’.\n\nDownload the dataset available from the course Moodle page under the ‘Lab Activities’ tile.\nUnzip the file. You can see that the following tables in CSV files:\n\nTrips\nPersons\nHouseholds\nVehicles\nDays\n\nMove the files into your RStudio project. It is a good practice to keep inputs, code, and outputs separate. I suggest you to create a folder called data in your project folder and paste all the survey files in it.\n\nThe codebook is available on Moodle, too. Additional information offered by the source are available at https://www.psrc.org/our-work/household-travel-survey-program.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1. Working environment and formatting data</span>"
    ]
  },
  {
    "objectID": "session-1.html#hands-on",
    "href": "session-1.html#hands-on",
    "title": "1  Session 1. Working environment and formatting data",
    "section": "1.3 Hands on",
    "text": "1.3 Hands on\nThe objective of this first part is to reproduce the results shown below. For now, I encourage you to type the code, rather than copying, this can help you to actively think of what the code is doing.\nAs a first step, create an RMardown/Quarto file, and save it in your RStudio project (remove all the default contents, if included).\nFirst, load the packages required for the analysis (install if needed).\n\n# for data manipulation and visualization\nlibrary(tidyverse) \n\nThen, read the ‘Household’ table from the CSV file. Make sure to adjust the directory to match the location where you save the Survey files.\n\n# Reading the Household data\nhouseholds &lt;- read_csv(\"data/Households.csv\")\n\nA quick check. How many rows and columns are there in the dataset?\n\nnrow(households)\n\n[1] 12118\n\nncol(households)\n\n[1] 69\n\n\nThat’s many columns. Check the first first 20 variable names in the houshold table.\n\nhead(colnames(household), 20)\n\n[1] \"family\"      \"dob_child1\"  \"dob_child2\"  \"name_child1\" \"name_child2\"\n\n\nWhat do all these names mean?!\nDownload the dataset codebook from Moodle, and keep it in your project folder. Spend 5-10 minutes to explore the structure and get familiar with the contents of the dataset.\n\n1.3.1 Subsetting\nLets subset some variables of interest from the Household table. Also, keep the records for the year 2023 only.\n\n# Subset the data to select only the variables we need for the year 2023\nhousehold_subset &lt;- households %&gt;%\n  filter(survey_year == 2023) %&gt;%\n  select(\n    household_id, hhincome_broad, hhsize, \n    home_county, vehicle_count, hh_race_category, \n    numadults, numchildren, numworkers\n  )\n\nWe can glimpse the data to see the type of the variables are included.\n\nglimpse(household_subset)\n\nRows: 3,870\nColumns: 9\n$ household_id     &lt;dbl&gt; 23007083, 23007112, 23003118, 23007129, 23007152, 230…\n$ hhincome_broad   &lt;chr&gt; \"$75,000-$99,999\", \"$50,000-$74,999\", \"$25,000-$49,99…\n$ hhsize           &lt;chr&gt; \"3 people\", \"2 people\", \"1 person\", \"2 people\", \"3 pe…\n$ home_county      &lt;chr&gt; \"Kitsap County\", \"King County\", \"King County\", \"King …\n$ vehicle_count    &lt;chr&gt; \"4 vehicles\", \"1 vehicle\", \"1 vehicle\", \"1 vehicle\", …\n$ hh_race_category &lt;chr&gt; \"AANHPI non-Hispanic\", \"White non-Hispanic\", \"AANHPI …\n$ numadults        &lt;chr&gt; \"3 adults\", \"2 adults\", \"1 adult\", \"2 adults\", \"2 adu…\n$ numchildren      &lt;chr&gt; \"0 children\", \"0 children\", \"0 children\", \"0 children…\n$ numworkers       &lt;chr&gt; \"3 workers\", \"2 workers\", \"1 worker\", \"0 workers\", \"2…",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1. Working environment and formatting data</span>"
    ]
  },
  {
    "objectID": "session-1.html#formating-variables",
    "href": "session-1.html#formating-variables",
    "title": "1  Session 1. Working environment and formatting data",
    "section": "1.4 Formating variables",
    "text": "1.4 Formating variables\nSome columns are not in it’s appropriate type, as they mix text with numbers. Thus, we will have to extract the numbers and set categorical variables to factors.\n\nhousehold_subset &lt;- household_subset %&gt;%\n  mutate(\n    hhsize = parse_number(hhsize),\n    vehicle_count = parse_number(vehicle_count),\n    numadults = parse_number(numadults),\n    numchildren = parse_number(numchildren),\n    numworkers = parse_number(numworkers)\n  )\n\nCategorical variables as factors.\n\n# Convert all required variables to factors\nhousehold_subset &lt;- household_subset %&gt;%\n  mutate(\n    across(c(hhincome_broad, home_county, hh_race_category), \n    factor\n))\n\nLet’s glimpse the formatted variables.\n\nglimpse(household_subset)\n\nRows: 3,870\nColumns: 9\n$ household_id     &lt;dbl&gt; 23007083, 23007112, 23003118, 23007129, 23007152, 230…\n$ hhincome_broad   &lt;fct&gt; \"$75,000-$99,999\", \"$50,000-$74,999\", \"$25,000-$49,99…\n$ hhsize           &lt;dbl&gt; 3, 2, 1, 2, 3, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2,…\n$ home_county      &lt;fct&gt; Kitsap County, King County, King County, King County,…\n$ vehicle_count    &lt;dbl&gt; 4, 1, 1, 1, 1, 2, 1, 1, 2, 0, 1, 2, 1, 2, 1, 3, 1, 2,…\n$ hh_race_category &lt;fct&gt; AANHPI non-Hispanic, White non-Hispanic, AANHPI non-H…\n$ numadults        &lt;dbl&gt; 3, 2, 1, 2, 2, 2, 1, 2, 2, 1, 1, 1, 1, 1, 2, 2, 1, 2,…\n$ numchildren      &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,…\n$ numworkers       &lt;dbl&gt; 3, 2, 1, 0, 2, 1, 1, 2, 1, 1, 1, 0, 1, 1, 2, 0, 1, 1,…\n\n\nThe income bands are ordered categories. However, they are unordered. Rearrenge them as following:\nFormat the order of household income labels\n\n# Type ordered income labs\nincome_labs &lt;- c(\n  \"Under $25,000\", \n  \"$25,000-$49,999\", \n  \"$50,000-$74,999\", \n  \"$75,000-$99,999\", \n  \"$100,000-$199,999\", \n  \"$200,000 or more\"\n)\n\nhousehold_subset &lt;- household_subset %&gt;% \n  mutate(hhincome_broad = factor(hhincome_broad, levels = income_labs))",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1. Working environment and formatting data</span>"
    ]
  },
  {
    "objectID": "session-1.html#descriptive-statistics",
    "href": "session-1.html#descriptive-statistics",
    "title": "1  Session 1. Working environment and formatting data",
    "section": "1.5 Descriptive statistics",
    "text": "1.5 Descriptive statistics\nWe can have a quick and dirt descriptive statistics overview by producing a default summary.\n\nsummary(household_subset)\n\n  household_id                hhincome_broad     hhsize     \n Min.   :23000173   Under $25,000    : 402   Min.   :1.000  \n 1st Qu.:23073829   $25,000-$49,999  : 490   1st Qu.:1.000  \n Median :23154700   $50,000-$74,999  : 509   Median :2.000  \n Mean   :23173722   $75,000-$99,999  : 423   Mean   :1.954  \n 3rd Qu.:23265921   $100,000-$199,999:1065   3rd Qu.:2.000  \n Max.   :23423045   $200,000 or more : 605   Max.   :9.000  \n                    NA's             : 376                  \n           home_county   vehicle_count  \n King County     :2793   Min.   :0.000  \n Kitsap County   : 215   1st Qu.:1.000  \n Pierce County   : 416   Median :1.000  \n Snohomish County: 446   Mean   :1.389  \n                         3rd Qu.:2.000  \n                         Max.   :8.000  \n                                        \n                               hh_race_category   numadults    \n AANHPI non-Hispanic                   : 667    Min.   :1.000  \n Black or African American non-Hispanic: 121    1st Qu.:1.000  \n Hispanic                              : 260    Median :2.000  \n Missing/No response                   : 640    Mean   :1.683  \n Some Other Races non-Hispanic         : 132    3rd Qu.:2.000  \n White non-Hispanic                    :2050    Max.   :9.000  \n                                                               \n  numchildren       numworkers   \n Min.   :0.0000   Min.   :0.000  \n 1st Qu.:0.0000   1st Qu.:0.000  \n Median :0.0000   Median :1.000  \n Mean   :0.2716   Mean   :1.107  \n 3rd Qu.:0.0000   3rd Qu.:2.000  \n Max.   :5.0000   Max.   :6.000  \n                                 \n\n\nFor tailored descriptive statistic summarise, we can do the following:\n\nhousehold_subset %&gt;% \n  summarise(hhsize_mean = mean(hhsize), hhsize_sd = sd(hhsize))\n\n# A tibble: 1 × 2\n  hhsize_mean hhsize_sd\n        &lt;dbl&gt;     &lt;dbl&gt;\n1        1.95      1.11\n\n\nAn we can easily extend this breaking down the descriptive statistics by group a categorical characteristic of the household. In the example below, by the home County.\n\n household_subset %&gt;% \n  group_by(home_county) %&gt;% \n  summarise(mean = mean(hhsize), sd = sd(hhsize))\n\n# A tibble: 4 × 3\n  home_county       mean    sd\n  &lt;fct&gt;            &lt;dbl&gt; &lt;dbl&gt;\n1 King County       1.87  1.06\n2 Kitsap County     2.18  1.12\n3 Pierce County     2.09  1.17\n4 Snohomish County  2.25  1.26\n\n\nTry summarising by hh_race_category.\nThese examples provide useful and flexible summaries. However, it’d require some work to format it for a final output. Packages that can help you creating descriptive statistic tables are: gtsummary, modelsummary, summarytools.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1. Working environment and formatting data</span>"
    ]
  },
  {
    "objectID": "session-1.html#missing-data",
    "href": "session-1.html#missing-data",
    "title": "1  Session 1. Working environment and formatting data",
    "section": "1.6 Missing data",
    "text": "1.6 Missing data\nExamine the variable hh_race_category. Are the missing values?\nFollowing the conventions in R, missing values should be treated as NA. Format these as appropriate using the fct_recode().\nThere are many reasons why some data might be missing.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1. Working environment and formatting data</span>"
    ]
  },
  {
    "objectID": "session-1.html#wraping-up",
    "href": "session-1.html#wraping-up",
    "title": "1  Session 1. Working environment and formatting data",
    "section": "1.7 Wraping up",
    "text": "1.7 Wraping up\n\n1.7.1 Write the formated subset\nWe will finish this session here.\nWe will write the formatted household subset. So, you do not have to go over the same steps in the next session.\n\nwrite_rds(household_subset,file = 'data/household_subset.RDS')\n\n\n\n1.7.2 Final reflections\n\nWhat year are you including in the data?\nWhat level of unit is represented in the table you prepared?",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1. Working environment and formatting data</span>"
    ]
  },
  {
    "objectID": "session-2.html",
    "href": "session-2.html",
    "title": "2  Session 2 Data structures and descriptive statistics",
    "section": "",
    "text": "2.1 Preliminaries\nFirst, load the packages required for the analysis (install if needed).\nlibrary(tidyverse) # for data manipulation and visualization\nWe will pick it up where we left it last time, and read the household data in the RDS file you prepared. If you haven’t completed Session 1, go to it and complete it before continuing here.\nhousehold_subset &lt;- read_rds('data/household_subset.RDS')\nYou will also need the ‘Trips’ table for this session. As we will combine the information from households and trips reported by each household.\ntrips &lt;- read_csv(\"data/Trips.csv\")",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 Data structures and descriptive statistics</span>"
    ]
  },
  {
    "objectID": "session-2.html#format-trips-table",
    "href": "session-2.html#format-trips-table",
    "title": "2  Session 2 Data structures and descriptive statistics",
    "section": "2.2 Format Trips table",
    "text": "2.2 Format Trips table\nSubset trips for the year 2023 and keep only variables of interest.\n\ntrips_subset &lt;- trips %&gt;%\n  filter(survey_year == 2023) %&gt;%\n  select(\n    household_id, person_id, trip_id, \n    mode_class, depart_date, depart_time_hour, \n    arrival_time_hour, origin_county, dest_county, \n    origin_purpose_cat, dest_purpose_cat, distance_meters,\n    duration_minutes\n)\n\nALWAYS CHECK THE SUBSET!!! What would you check?\n\nnrow(trips_subset)\n\n[1] 56704\n\nncol(trips_subset)\n\n[1] 13\n\n\n\ncolnames(trips_subset)\n\n [1] \"household_id\"       \"person_id\"          \"trip_id\"           \n [4] \"mode_class\"         \"depart_date\"        \"depart_time_hour\"  \n [7] \"arrival_time_hour\"  \"origin_county\"      \"dest_county\"       \n[10] \"origin_purpose_cat\" \"dest_purpose_cat\"   \"distance_meters\"   \n[13] \"duration_minutes\"  \n\nhead(trips_subset)\n\n# A tibble: 6 × 13\n  household_id  person_id       trip_id mode_class  depart_date depart_time_hour\n         &lt;dbl&gt;      &lt;dbl&gt;         &lt;dbl&gt; &lt;chr&gt;       &lt;date&gt;                 &lt;dbl&gt;\n1     23000858 2300085801 2300085801002 Walk        2023-04-27                16\n2     23000466 2300046601 2300046601004 Walk        2023-04-20                14\n3     23001235 2300123501 2300123501011 Drive HOV3+ 2023-04-27                19\n4     23000466 2300046601 2300046601005 Walk        2023-04-20                14\n5     23000858 2300085801 2300085801003 Drive HOV2  2023-04-27                19\n6     23001235 2300123501 2300123501012 Drive HOV3+ 2023-04-27                19\n# ℹ 7 more variables: arrival_time_hour &lt;dbl&gt;, origin_county &lt;chr&gt;,\n#   dest_county &lt;chr&gt;, origin_purpose_cat &lt;chr&gt;, dest_purpose_cat &lt;chr&gt;,\n#   distance_meters &lt;dbl&gt;, duration_minutes &lt;dbl&gt;\n\n\nTransform the categorical variables as factor.\n\ntrips_subset &lt;- trips_subset %&gt;%\n  mutate(\n    across(\n      c(mode_class, origin_county, dest_county, origin_purpose_cat, dest_purpose_cat), \n      as.factor)\n  )",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 Data structures and descriptive statistics</span>"
    ]
  },
  {
    "objectID": "session-2.html#trip-descriptive-statistics",
    "href": "session-2.html#trip-descriptive-statistics",
    "title": "2  Session 2 Data structures and descriptive statistics",
    "section": "2.3 Trip descriptive statistics",
    "text": "2.3 Trip descriptive statistics\nCalculate a summary at the person-level. Include the number of trips, total travel distance and travel time for each person, within household.\n\nperson_summary &lt;- trips_subset %&gt;%\n  group_by(person_id, household_id) %&gt;%\n  summarise(\n    number_of_trips = n(),\n    # Distance in kilometres\n    total_travel_distance = sum(distance_meters / 1000),\n    total_travel_time = sum(duration_minutes)\n  ) %&gt;% \n  ungroup()\n\nNow, you need to normalise the total distance and travel time based on the number of trips by calculating the average values.\n\nperson_summary &lt;- person_summary %&gt;% \n  mutate(\n    avg_trip_distance = total_travel_distance / number_of_trips,\n    avg_trip_duration = total_travel_time / number_of_trips\n  )\n\nCreate a quick summary\n\nsummary(person_summary)\n\n   person_id          household_id      number_of_trips   total_travel_distance\n Min.   :2.300e+09   Min.   :23000173   Min.   :  2.000   Min.   :    0.066    \n 1st Qu.:2.307e+09   1st Qu.:23070124   1st Qu.:  2.000   1st Qu.:   11.425    \n Median :2.315e+09   Median :23153760   Median :  4.000   Median :   29.122    \n Mean   :2.317e+09   Mean   :23170889   Mean   :  9.509   Mean   :  124.803    \n 3rd Qu.:2.326e+09   3rd Qu.:23264171   3rd Qu.:  8.000   3rd Qu.:   79.841    \n Max.   :2.342e+09   Max.   :23423045   Max.   :307.000   Max.   :18795.769    \n                                                          NA's   :1            \n total_travel_time avg_trip_distance  avg_trip_duration\n Min.   :   2.0    Min.   :   0.033   Min.   :  1.00   \n 1st Qu.:  48.0    1st Qu.:   2.925   1st Qu.: 12.97   \n Median :  88.0    Median :   5.951   Median : 19.00   \n Mean   : 196.7    Mean   :  14.324   Mean   : 24.67   \n 3rd Qu.: 205.0    3rd Qu.:  12.080   3rd Qu.: 30.00   \n Max.   :2573.0    Max.   :2697.030   Max.   :270.50   \n NA's   :1         NA's   :1          NA's   :1        \n\n\nDo the values make sense?\nThere are some extreme values in the number of trips (more than 300?!) and average trip distance (more thank 2,000 KM?). We will remove them for this analysis. In practice, we should carefully make choices and justify them.\n\nperson_summary &lt;- person_summary %&gt;% \n  filter(number_of_trips &lt; 50 & avg_trip_distance &lt; 100)",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 Data structures and descriptive statistics</span>"
    ]
  },
  {
    "objectID": "session-2.html#visualising-data",
    "href": "session-2.html#visualising-data",
    "title": "2  Session 2 Data structures and descriptive statistics",
    "section": "3.1 Visualising data",
    "text": "3.1 Visualising data\nUnivariate distribution: Check number of trips.\n\nperson_summary %&gt;% \n  ggplot(aes(number_of_trips)) +\n  geom_histogram() +\n  labs(\n    x = \"Total number of trips\",\n    y = \"Count\"\n  )\n\n\n\n\n\n\n\n\nWhat is a suitable alternative to a histogram to visualise a variable distribution?\nBivariate plot: Visualise average number of trip and HH income.\n\nperson_summary %&gt;% \n  ggplot(aes(number_of_trips, hhincome_broad)) +\n  geom_boxplot() +\n  labs(\n    x = \"Total trips\",\n    y = \"Household income\"\n  )\n\n\n\n\n\n\n\n\nVisualise average trip distance and HH income.\n\nperson_summary %&gt;% \n  ggplot(aes(avg_trip_distance, hhincome_broad)) +\n  geom_boxplot() +\n  labs(\n    x = \"Average trip distance in kilometres\",\n    y = \"Household income\"\n  )",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 Data structures and descriptive statistics</span>"
    ]
  },
  {
    "objectID": "session-3.html",
    "href": "session-3.html",
    "title": "3  Session 3: Linear regression",
    "section": "",
    "text": "3.1 Preliminaries\nFor today’s session we will need the following packages. Install them using the install.packages() function if you don’t have them available.\n# Packages \nlibrary(tidyverse)\n\n# Today we will also be using\n# This is helpful to check the assumptions of linear models\nlibrary(performance)\nNow, we will read the data in to the R session. We will use three tables from the travel survey dataset.\n# Read data\ntrips &lt;- read_csv('data/Trips.csv')\npersons &lt;- read_csv('data/Persons.csv')\nhouseholds &lt;- read_csv('data/Households.csv')",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3: Linear regression</span>"
    ]
  },
  {
    "objectID": "session-3.html#preliminaries",
    "href": "session-3.html#preliminaries",
    "title": "3  Session 3: Linear regression",
    "section": "",
    "text": "You should have created an Rstudio project in Week 1. If you have not do so yet, I encourage you to return to a previous chapter for detailed instructions on data availability and structure.\nGet familiar with the dataset. If you haven’t do so, take a few minutes to look at the introduction to it is available at: https://www.psrc.org/media/3248, and the data dictionary, which is avaiable on Moodle, too.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3: Linear regression</span>"
    ]
  },
  {
    "objectID": "session-3.html#estimating-the-average-miles-driven-per-household-a-day",
    "href": "session-3.html#estimating-the-average-miles-driven-per-household-a-day",
    "title": "3  Session 3: Linear regression",
    "section": "3.2 Estimating the average miles driven per household a day",
    "text": "3.2 Estimating the average miles driven per household a day\nIn this first example, we are interested to know about driving behaviour. So, we will use the ‘trips’ table to summarise information, and supplement it with information from the ‘households’ table by joining these together.\nFirst, we will subset the trips which primary trip mode is ‘Drive’ only, for the year 2023.\n\ntrips_drive &lt;- trips %&gt;% \n  filter(grepl('Drive', mode_class) & survey_year == '2023')\n\nWhat is function grepl() doing in the previous chunk? How many observations were kept in for trips driving compared to all of the trips?\nThe we will create key variables relating to driving behaviour at the household level. We are crucially interested in the average miles driven a day per household. To do so, we summarise the information at the household level. Effectively, we sum the trip distance, and divide it by the number of days with trips recorded in the survey to obtain the average miles driven.\n\nmiles_driven_hh &lt;- trips_drive %&gt;% \n  group_by(household_id) %&gt;% \n  summarise(\n    total_miles_hh = sum(distance_miles, na.rm = TRUE),\n    num_days_trips = max(daynum),\n    avg_miles_driven = total_miles_hh / num_days_trips\n  )\n\nAs we now have a summary per household, we can join more information about the household using the ‘household_id’ unique identifier in the left_join() function.\n\n# Join household data\nmiles_driven_hh &lt;- miles_driven_hh %&gt;% \n  left_join(households, by = 'household_id')",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3: Linear regression</span>"
    ]
  },
  {
    "objectID": "session-3.html#exploratory-analysis",
    "href": "session-3.html#exploratory-analysis",
    "title": "3  Session 3: Linear regression",
    "section": "3.3 Exploratory analysis",
    "text": "3.3 Exploratory analysis\nIn this analysis, we will focus on respondents having more than 0 miles driven. In other words, only households who have reported to have droven in the survey.\n\n# Subset\nmiles_driven_hh &lt;- miles_driven_hh %&gt;% \n  filter(avg_miles_driven &gt; 0)\n\nLte’s visualize distribution of miles driven.\n\nmiles_driven_hh %&gt;% \n  ggplot(aes(avg_miles_driven)) +\n  geom_density() + \n  labs(x = 'Avg. miles driven per household', y = 'Density') +\n  theme_minimal()\n\n\n\n\n\n\n\n\nSomething strange?…\nLet’s limit the maximum average of vehicle miles driven. The threshold chosen should be justified, e.g. preliminary information in the context, literature, or external empirical references.\n\nmiles_driven_hh &lt;- miles_driven_hh %&gt;% \n  filter(avg_miles_driven &lt;= 500)\n\n\n3.3.1 Bivariate analysis\nWe will transform the numchildren variables to factor, treating it as a categorical. Why? Are there options?\n\nmiles_driven_hh &lt;- miles_driven_hh %&gt;% \n  mutate(numchildren = factor(numchildren))\n\nPlot the distribution of the average vehicle miles driven by groups.\n\n# Plot distribution by groups\nmiles_driven_hh %&gt;% \n  ggplot(aes(numchildren, avg_miles_driven)) +\n  geom_boxplot() +\n  labs(x = 'Number of children', y = 'Avg. miles driven per household') +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n3.3.2 How does working from home relates to vehicle miles driven?\nIn this session, the aim is to explain wow working from home relates to vehicle miles driven. For this, we will need to get the information about individuals in the household. We can do this using the ‘persons’ table.\nFirst, look at the categories in the ‘workplace’ variable\n\ncount(persons, workplace)\n\n# A tibble: 7 × 2\n  workplace                                                      n\n  &lt;chr&gt;                                                      &lt;int&gt;\n1 At home (telecommute or self-employed with home office)     2118\n2 Drives for a living (e.g., bus driver, salesperson)          257\n3 Missing: Skip Logic                                         7652\n4 Telework some days and travel to a work location some days  1554\n5 Usually the same location (outside home)                    8184\n6 Workplace regularly varies (different offices or jobsites)  1519\n7 &lt;NA&gt;                                                        2366\n\n\nThen, we create a new binary variable which identifies people working from home.\n\n# Create a binary variable indicating if anyone in works from home or teleworks\npersons &lt;- persons %&gt;% \n  mutate(\n    work_from_home = \n      ifelse(\n        # If the variable workplace contains 'At home' or 'Telework'\n        grepl('At home|Telework', workplace), \n        # return 'Yes'.\n        'Yes',\n        # Otherwise, we set it to 'No'\n        'No'\n))\n\nTo help you going about the previous code, you can read for more information about the conditional function typing ?ifelse in your R console.\nThen, we summarise the information at the household level summing the number of individuals at the household which work from home (‘pers_work_home’). Also, we will create another variable which checks if at leas one person works from home, i.e. ‘work_from_home’.\n\nwork_from_home &lt;- persons %&gt;% \n  group_by(household_id) %&gt;% \n  summarise(\n    pers_work_home = sum(work_from_home == 'Yes', na.rm = TRUE),\n    any_work_home = ifelse(pers_work_home &gt; 0, 'Yes', 'No')\n  )\n\nNow, we can join this new information to the data at the household level.\n\n# Join to subset\nmiles_driven_hh &lt;- miles_driven_hh %&gt;% \n  left_join(work_from_home, by = 'household_id')\n\nIs there any visual difference at the household level?\n\n# Visualise\nmiles_driven_hh %&gt;% \n  ggplot(aes(any_work_home, avg_miles_driven)) +\n  geom_boxplot() + \n  labs(\n    x = 'Any working from home', \n    y = 'Avg. miles driven per household'\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhat about the descriptive statistics?\n\nmiles_driven_hh %&gt;% \n  group_by(any_work_home) %&gt;% \n  summarise(\n    miles_driven_mean = mean(avg_miles_driven),\n    miles_driven_sd = sd(avg_miles_driven)\n  )\n\n# A tibble: 2 × 3\n  any_work_home miles_driven_mean miles_driven_sd\n  &lt;chr&gt;                     &lt;dbl&gt;           &lt;dbl&gt;\n1 No                         37.2            48.1\n2 Yes                        37.4            42.3",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3: Linear regression</span>"
    ]
  },
  {
    "objectID": "session-3.html#evaluate-the-difference-in-a-linear-regression-analysis",
    "href": "session-3.html#evaluate-the-difference-in-a-linear-regression-analysis",
    "title": "3  Session 3: Linear regression",
    "section": "3.4 Evaluate the difference in a linear regression analysis",
    "text": "3.4 Evaluate the difference in a linear regression analysis\nLet’ see what a simple linear model suggest.\n\nmiles_driven_hh %&gt;% \n  lm(avg_miles_driven ~ any_work_home, .) %&gt;% \n  summary()\n\n\nCall:\nlm(formula = avg_miles_driven ~ any_work_home, data = .)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-37.27 -26.84 -13.48  10.05 448.62 \n\nCoefficients:\n                 Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       37.1775     1.1546  32.200   &lt;2e-16 ***\nany_work_homeYes   0.1998     1.7202   0.116    0.908    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 45.6 on 2837 degrees of freedom\nMultiple R-squared:  4.755e-06, Adjusted R-squared:  -0.0003477 \nF-statistic: 0.01349 on 1 and 2837 DF,  p-value: 0.9075\n\n\nAre there potential problems with this model or results?\nWe will add socio-demographic variables as controls, e.g. size of the household matters!\nFirst, we format the order of household income labels and convert size of household from character to numeric. This will ease interpretations.\n\n# Income labels\nincome_labs &lt;- c(\n  \"Under $25,000\", \n  \"$25,000-$49,999\", \n  \"$50,000-$74,999\", \n  \"$75,000-$99,999\", \n  \"$100,000-$199,999\", \n  \"$200,000 or more\"\n)\nmiles_driven_hh &lt;- miles_driven_hh %&gt;% \n  mutate(\n    # Income labels\n    hhincome_broad = factor(hhincome_broad, income_labs),\n    # Household size as integer\n    hhsize_int = readr::parse_number(hhsize, \"\\\\d+\"),\n    # Children at household as binary\n    children_binary = ifelse(numchildren == '0 children', 'No', 'Yes')\n)\n\nWe run the multiple regression and print results.\n\nm1 &lt;- miles_driven_hh %&gt;% \n  lm(\n    avg_miles_driven ~ \n      any_work_home + hhsize_int + hhincome_broad + children_binary, .\n  )\nsummary(m1)\n\n\nCall:\nlm(formula = avg_miles_driven ~ any_work_home + hhsize_int + \n    hhincome_broad + children_binary, data = .)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-89.21 -22.86 -10.41   8.87 434.78 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                        4.927      3.609   1.365 0.172338    \nany_work_homeYes                  -8.301      1.933  -4.295 1.81e-05 ***\nhhsize_int                        10.952      1.084  10.107  &lt; 2e-16 ***\nhhincome_broad$25,000-$49,999      5.298      4.069   1.302 0.193021    \nhhincome_broad$50,000-$74,999     13.181      3.974   3.317 0.000923 ***\nhhincome_broad$75,000-$99,999     10.543      4.104   2.569 0.010254 *  \nhhincome_broad$100,000-$199,999   19.243      3.716   5.178 2.41e-07 ***\nhhincome_broad$200,000 or more    13.724      4.054   3.386 0.000721 ***\nchildren_binaryYes                 1.628      3.092   0.526 0.598643    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 44.44 on 2562 degrees of freedom\n  (268 observations deleted due to missingness)\nMultiple R-squared:  0.1021,    Adjusted R-squared:  0.09928 \nF-statistic: 36.41 on 8 and 2562 DF,  p-value: &lt; 2.2e-16\n\n\nDiscuss the model results with your tutor.",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3: Linear regression</span>"
    ]
  },
  {
    "objectID": "session-3.html#does-it-meet-the-model-assumptions-lets-check-the-model",
    "href": "session-3.html#does-it-meet-the-model-assumptions-lets-check-the-model",
    "title": "3  Session 3: Linear regression",
    "section": "3.5 Does it meet the model assumptions? … Lets check the model",
    "text": "3.5 Does it meet the model assumptions? … Lets check the model\nCheck model 1 assumptions. Type ‘y’ when you are asked about updating extensions.\n\ncheck_model(m1)",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3: Linear regression</span>"
    ]
  },
  {
    "objectID": "session-3.html#a-log-linear-model",
    "href": "session-3.html#a-log-linear-model",
    "title": "3  Session 3: Linear regression",
    "section": "3.6 A log-linear model",
    "text": "3.6 A log-linear model\nLets try to fit the model in the log-linear form.\n\nm2 &lt;- miles_driven_hh %&gt;% \n  mutate(log_miles_driven = log(avg_miles_driven)) %&gt;% \n  lm(\n    log_miles_driven ~ \n      any_work_home + hhsize_int + hhincome_broad + children_binary, .\n)\n\nAnd check the model again\n\ncheck_model(m2)\n\n\n\n\n\n\n\n\nWhat are the interpretations of the log-linear model? Think of coefficients and overall performance.\n\nsummary(m2)\n\n\nCall:\nlm(formula = log_miles_driven ~ any_work_home + hhsize_int + \n    hhincome_broad + children_binary, data = .)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-5.0276 -0.6560  0.1140  0.7483  3.5693 \n\nCoefficients:\n                                Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)                      1.86366    0.09244  20.161  &lt; 2e-16 ***\nany_work_homeYes                -0.23932    0.04951  -4.834 1.42e-06 ***\nhhsize_int                       0.30433    0.02776  10.964  &lt; 2e-16 ***\nhhincome_broad$25,000-$49,999    0.29811    0.10423   2.860  0.00427 ** \nhhincome_broad$50,000-$74,999    0.57722    0.10179   5.671 1.58e-08 ***\nhhincome_broad$75,000-$99,999    0.59114    0.10511   5.624 2.07e-08 ***\nhhincome_broad$100,000-$199,999  0.83933    0.09518   8.818  &lt; 2e-16 ***\nhhincome_broad$200,000 or more   0.77063    0.10383   7.422 1.56e-13 ***\nchildren_binaryYes               0.00540    0.07921   0.068  0.94565    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 1.138 on 2562 degrees of freedom\n  (268 observations deleted due to missingness)\nMultiple R-squared:  0.1396,    Adjusted R-squared:  0.1369 \nF-statistic: 51.95 on 8 and 2562 DF,  p-value: &lt; 2.2e-16\n\n\nWhich model would you choose? Are the any other potentially omitted variables?",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3: Linear regression</span>"
    ]
  },
  {
    "objectID": "session-4.html",
    "href": "session-4.html",
    "title": "4  Session 4: Logistic regression Part 1",
    "section": "",
    "text": "4.1 Preliminaries\nWe will continue to work with the Puget Sound Household Travel Survey. If you do not have the data or project set for this, please check the preliminary instructions for Week 1.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4: Logistic regression Part 1</span>"
    ]
  },
  {
    "objectID": "session-4.html#preliminaries",
    "href": "session-4.html#preliminaries",
    "title": "4  Session 4: Logistic regression Part 1",
    "section": "",
    "text": "4.1.1 Data manipulation\nFor today’s session we will need the following packages (Make sure that you have these packages installed in your machine, if not install them using the install.packages() function).\n\n# Packages \nlibrary(tidyverse) # For data manipulation\nlibrary(gtsummary) # Descriptive statistics\nlibrary(performance) # Model checks\n\nYou will also need to install a helper package: install.packages(\"broom.helpers\").\nNow, we will read the data in to the R session. You will need information from trips, persons, and households tables.\n\n# Load data\nhouseholds &lt;- read_csv('data/Households.csv')\npersons &lt;- read_csv('data/Persons.csv')\ntrips &lt;- read_csv('data/Trips.csv')\n\nLet’s work with data for the year 2023 only.\n\n# Filter data for the year 2023 only\nhouseholds &lt;- households %&gt;% filter(survey_year == 2023)\ntrips &lt;- trips %&gt;% filter(survey_year == 2023)\npersons &lt;- persons %&gt;% filter(survey_year == 2023)\n\n\n\n4.1.2 Summarising data at the appropriate level\n\n\n\n\n\n\nWhat is the unit of observation?\n\n\n\nMany transport datasets have a relational (hierarchical) structure. For example, repeated measurements may be nested within individuals, individuals within household, or households within places.\nBefore analysing the data, you will need to summarise the relevant information at the appropriate level of this hierarchy—that is, for the unit of observation that matches your research question.\nAsk yourself:\n\nWhat does one row of data represent at this stage of the analysis?\nAre there multiple observations per unit?\nDo I need to aggregate, average, or otherwise summarise the data to move to a higher-level unit?\n\nTake a moment to identify the unit of observation you are working with as you go through the following sections.\n\n\nIn this activity, we focus on trips to work only.\n\ntrips_work &lt;- trips %&gt;% \n  filter('Work' == dest_purpose_cat)\n\nThe dependent variable we are interested in is whether the travellers used an active transport mode or not to travel to work. This is a binary variable. So, we will use logistic regression to identify the relationships with other variables.\nThe criterion for constructing this variable is that the person has made at least one trip to work during the reported period. This information is obtained at the trip level and then summarised at the person level, as shown below.\n\n# Summarise trip data at the person level\ntrips_summary &lt;- trips_work %&gt;% \n  group_by(person_id) %&gt;% \n  summarise(\n    # Count the number of work trips made using an active mode, e.g. bike or walk\n    n_active_trips = sum(grepl(\"Walk|Bike\", mode_class)),\n    # Create a binary variable, ie whether they made any active trips\n    active_binary = ifelse(n_active_trips &gt; 0, 1, 0),\n    \n    # Total distance travelled across all work trips\n    total_miles = sum(distance_miles, na.rm = TRUE),\n    # Number of days with observed trips\n    num_days_trips = max(daynum, na.rm = TRUE),\n    # Average distance travelled per day\n    avg_distance = total_miles / num_days_trips\n)\n\nRemember, persons can generate many trips a day and use different modes. This is why we need to summarise the trip data at the person level.\nWe keep persons who reported trips to work only, and whose distance is less than 50 miles. This can be a reasonable threshold, as we are interested in active transport modes. In practice, it is always important to justify choices, e.g. literature, expert opinion, or empirical evidence.\n\ntrips_summary &lt;- trips_summary %&gt;% \n  filter(num_days_trips &gt; 0 & avg_distance &lt; 50)\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat share of respondents use an active travel mode when travelling to work?\n\n\n\n\n4.1.3 Independent variables\nIn addition to the travel behaviour information, we want to know more about the demographics and characteristics of the household. Thus, we join the trip summary and household data at the person level.\n\npersons_main &lt;- persons %&gt;% \n  left_join(trips_summary, by = 'person_id') %&gt;% \n  left_join(households, by = 'household_id')\n\nNext, we format the independent variables and simplify some of these.\nWe will also create the key independent variable, namely: having free parking at work. We wish to examine if and how this is related to the use of active transport modes. We are also including further demographic control variables at the person and household level.\n\n# Write the name and appropriate order of the levels for hhincome_broad\nincome_labs &lt;- c(\n  \"Under $25,000\", \n  \"$25,000-$49,999\", \n  \"$50,000-$74,999\", \n  \"$75,000-$99,999\", \n  \"$100,000-$199,999\", \n  \"$200,000 or more\"\n)\n\n# Categories for higher education (explicit definition)\nhe_levs &lt;- c(\n  \"Bachelor degree\",\n  \"Graduate/post-graduate degree\",\n  \"Associates degree\"\n)\n\n# Format and create appropriate variables\npersons_main &lt;- persons_main %&gt;% \n  mutate(\n    # Whether person has free parking at work\n    free_parking = ifelse(commute_subsidy_3 == 'Selected', 'Yes', 'No'),\n    # Simplify education, e.g. graduate and bachelor vs all other\n    higher_education = ifelse(education %in% he_levs, 'Yes', 'No'),\n    children = ifelse(numchildren == '0 children', 'No', 'Yes'),\n    hhincome_broad = factor(hhincome_broad, levels = income_labs)\n)\n\nBefore moving to the analysis, we will keep only complete observations for the variables of interest. We will remove incomplete cases—that is, persons with missing values in any of the selected columns.\n\n\n\n\n\n\nBe careful: removing incomplete cases\n\n\n\nThis step requires caution, especially when working with supplementary variables where missing values may be expected.\nFor example, the variable age of children will naturally be missing for persons without children. Treating these cases as incomplete would incorrectly remove valid observations and result in the loss of valuable information.\nBefore dropping cases, always consider why values are missing and whether they are meaningful for the unit of observation and research question.\n\n\n\n# First, select variables of interest\npersons_main &lt;- persons_main %&gt;% \n  select(\n    active_binary, \n    free_parking,\n    avg_distance,\n    gender,\n    higher_education,\n    children,\n    hhincome_broad\n  )\n\n# Then, removing incomplete observations\npersons_main &lt;- persons_main %&gt;% \n  drop_na()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4: Logistic regression Part 1</span>"
    ]
  },
  {
    "objectID": "session-4.html#descriptive-statistics",
    "href": "session-4.html#descriptive-statistics",
    "title": "4  Session 4: Logistic regression Part 1",
    "section": "4.2 Descriptive statistics",
    "text": "4.2 Descriptive statistics\nBefore moving on to the modelling stage, always check that the descriptive statistics are plausible and consistent, i.e. do values make sense?!. This step helps you catch data issues, unexpected patterns, or coding errors early—before they affect your models. Let’s do that now.\nPrint a simple summary of the sample including all variables selected. We split the data according to the main dependent variable, whether respondents used active mode to travel to work.\npersons_main %&gt;% \n  tbl_summary(\n    by = active_binary,\n    statistic = all_continuous() ~ \"{mean} ± {sd}\"\n  ) %&gt;% \n  add_overall()\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOverall N = 1,6811\n0 N = 1,2381\n1 N = 4431\n\n\n\n\nfree_parking\n562 (33%)\n477 (39%)\n85 (19%)\n\n\navg_distance\n7 ± 7\n9 ± 8\n3 ± 4\n\n\ngender\n\n\n\n\n\n\n\n\n    Boy/Man (cisgender or transgender)\n787 (47%)\n562 (45%)\n225 (51%)\n\n\n    Girl/Woman (cisgender or transgender)\n772 (46%)\n583 (47%)\n189 (43%)\n\n\n    Non-binary/Something else fits better\n43 (2.6%)\n29 (2.3%)\n14 (3.2%)\n\n\n    Prefer not to answer\n79 (4.7%)\n64 (5.2%)\n15 (3.4%)\n\n\nhigher_education\n1,325 (79%)\n948 (77%)\n377 (85%)\n\n\nchildren\n384 (23%)\n326 (26%)\n58 (13%)\n\n\nhhincome_broad\n\n\n\n\n\n\n\n\n    Under $25,000\n56 (3.3%)\n29 (2.3%)\n27 (6.1%)\n\n\n    $25,000-$49,999\n191 (11%)\n132 (11%)\n59 (13%)\n\n\n    $50,000-$74,999\n237 (14%)\n177 (14%)\n60 (14%)\n\n\n    $75,000-$99,999\n203 (12%)\n143 (12%)\n60 (14%)\n\n\n    $100,000-$199,999\n598 (36%)\n467 (38%)\n131 (30%)\n\n\n    $200,000 or more\n396 (24%)\n290 (23%)\n106 (24%)\n\n\n\n1 n (%); Mean ± SD\n\n\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\n\nAre there noticeable differences between respondent groups?\nCan you make conclusions from this?\n\n\n\n\n4.2.1 Write the data\nBefore moving ahead, save the data subset with the variables that you have formatted and created. This will also be helpful for the next session.\n\nwrite_rds(persons_main,'data/persons_main.rds')",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4: Logistic regression Part 1</span>"
    ]
  },
  {
    "objectID": "session-4.html#logistic-regression",
    "href": "session-4.html#logistic-regression",
    "title": "4  Session 4: Logistic regression Part 1",
    "section": "4.3 Logistic regression",
    "text": "4.3 Logistic regression\nWe fit a logistic regression model to identify the relationships between the dependent variable (active travel) and key independent variable (having free parking at the workplace), including control variables, such as average trip distance, gender, higher education, presence of children at household, and household income.\nTo fit the logistic regression we use the glm() function and specify the family ‘binomial’. This uses the first category as the reference group (the group without the outcome) and treats the second category as the outcome group. In this case, ‘0’ = no active travel (the reference group), and ‘1’ = active travel. This is critical to make correct coefficient interpretations.\n\nlogit_model1 &lt;- glm(\n  active_binary ~ free_parking + avg_distance + gender + higher_education + children + hhincome_broad,\n  family = \"binomial\", \n  data = persons_main\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrint the results of the model in the log odds ratio scale.\n\nlogit_model1 %&gt;% \n  tbl_regression() %&gt;% \n  add_significance_stars(hide_ci = FALSE, hide_p = FALSE) %&gt;% \n  add_glance_table()\n\n\n\n\n  \n    \n      Characteristic\n      log(OR)1\n      SE\n      95% CI\n      p-value\n    \n  \n  \n    free_parking\n\n\n\n\n        No\n—\n—\n—\n\n        Yes\n-0.75***\n0.148\n-1.0, -0.47\n&lt;0.001\n    avg_distance\n-0.23***\n0.019\n-0.27, -0.20\n&lt;0.001\n    gender\n\n\n\n\n        Boy/Man (cisgender or transgender)\n—\n—\n—\n\n        Girl/Woman (cisgender or transgender)\n-0.52***\n0.132\n-0.78, -0.26\n&lt;0.001\n        Non-binary/Something else fits better\n-0.29\n0.393\n-1.1, 0.47\n0.5\n        Prefer not to answer\n-0.62\n0.335\n-1.3, 0.02\n0.064\n    higher_education\n\n\n\n\n        No\n—\n—\n—\n\n        Yes\n0.63***\n0.175\n0.30, 0.98\n&lt;0.001\n    children\n\n\n\n\n        No\n—\n—\n—\n\n        Yes\n-0.66***\n0.175\n-1.0, -0.33\n&lt;0.001\n    hhincome_broad\n\n\n\n\n        Under $25,000\n—\n—\n—\n\n        $25,000-$49,999\n-0.63\n0.349\n-1.3, 0.06\n0.072\n        $50,000-$74,999\n-0.74*\n0.353\n-1.4, -0.05\n0.036\n        $75,000-$99,999\n-0.71*\n0.354\n-1.4, -0.02\n0.045\n        $100,000-$199,999\n-0.84*\n0.332\n-1.5, -0.19\n0.012\n        $200,000 or more\n-0.59\n0.345\n-1.3, 0.09\n0.087\n    Null deviance\n1,939\n\n\n\n    Null df\n1,680\n\n\n\n    Log-likelihood\n-761\n\n\n\n    AIC\n1,548\n\n\n\n    BIC\n1,619\n\n\n\n    Deviance\n1,522\n\n\n\n    Residual df\n1,668\n\n\n\n    No. Obs.\n1,681\n\n\n\n  \n  \n    \n      Abbreviations: CI = Confidence Interval, OR = Odds Ratio, SE = Standard Error\n    \n  \n  \n    \n      1 *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001\n    \n  \n\n\n\n\nFor now, the interpretation of the coefficients is on the log-odds scale. For example, the coefficient for free parking at work is -0.75 and statistically significant. This means that having free parking is associated with a decrease in the log-odds of using active transport modes compared to people without free parking:\n\\[\n\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 \\cdot \\text{FreeParking} + \\dots\n\\]\nwhere \\(p\\) is the probability of using an active mode, and \\(\\beta_1 = -0.75\\).\nFor continuous variables: We expect the log-odds of using active transport modes to decrease by 0.24 for every additional mile to work. This is also a significant predictor.\n\n\n\n\n\n\nReflection\n\n\n\nCan you provide the interpretation for other coefficients?\n\n\n\n4.3.1 Final thoughts\nSo far, the results of the logistic regression are interpreted in terms of the log of the odds. We can exponentiate the coefficient to interpret it as an odds ratio, which is often more intuitive. This is what we will do in the next hands-on session, including the discussion of model fit measures and model assumption checks.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4: Logistic regression Part 1</span>"
    ]
  },
  {
    "objectID": "session-5.html",
    "href": "session-5.html",
    "title": "5  Session 5: Logistic regression Part 2",
    "section": "",
    "text": "5.1 Preliminaries\nIn today’s session we will continue learning about logistic regression. Last time, we learned more about suitable outcome variables for this type of model, i.e. binary ones (which only have two mutually exclusive categories). Also, we fitted a logistic regression model and were introduced to coefficient interpretation. We made interpretations in the log of the odds. Today, we will learn about coefficient interpretations in the odds ratio scale. Also, we will learn more about model goodness-of-fit and checking model assumptions.\nFor today’s session we will need the following packages. You needed them for the previous lab.\n# Packages \nlibrary(tidyverse) # For data manipulation\nlibrary(gtsummary) # Descriptive statistics\nlibrary(performance) # Model checks\nWe will continue to work with the data subset that you created in the last session, which includes information about commuting to work trips at the individual level.\npersons_main &lt;- readRDS('data/persons_main.RDS')\nLet’s take a quick look to refresh our understanding of the data’s contents and structure.\nglimpse(persons_main)\n\nRows: 1,681\nColumns: 7\n$ active_binary    &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ free_parking     &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\"…\n$ avg_distance     &lt;dbl&gt; 2.313454, 1.360803, 4.188145, 2.065438, 2.835317, 3.9…\n$ gender           &lt;chr&gt; \"Girl/Woman (cisgender or transgender)\", \"Boy/Man (ci…\n$ higher_education &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes…\n$ children         &lt;chr&gt; \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"N…\n$ hhincome_broad   &lt;fct&gt; \"$100,000-$199,999\", \"$25,000-$49,999\", \"$200,000 or …",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5: Logistic regression Part 2</span>"
    ]
  },
  {
    "objectID": "session-5.html#estimating-a-logistic-regression-model",
    "href": "session-5.html#estimating-a-logistic-regression-model",
    "title": "5  Session 5: Logistic regression Part 2",
    "section": "5.2 Estimating a logistic regression model",
    "text": "5.2 Estimating a logistic regression model\nWe start by estimating the same logistic regression model that we discussed in Session 4. Here, we aim to identify the relationships between having free parking at work (key independent variable) and persons engaging in active travel to work (dependent variable), including control variables, such as average trip distance, gender, higher education, presence of children at household, and household income.\nTo estimate the logistic regression we use the glm() function and specify the family ‘binomial’. The glm() function uses the first category as the reference group (the group without the outcome) and treats the second category as the outcome group. In this case, ‘1’ = active travel, and ‘0’ = no active travel (the reference group). This is critical to make correct coefficient interpretations.\n\nlogit_model1 &lt;- glm(\n  active_binary ~ free_parking + avg_distance + gender + higher_education + children + hhincome_broad,\n  family = \"binomial\", \n  data = persons_main\n)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5: Logistic regression Part 2</span>"
    ]
  },
  {
    "objectID": "session-5.html#interpreting-exponentied-coefficients-odds-ratio",
    "href": "session-5.html#interpreting-exponentied-coefficients-odds-ratio",
    "title": "5  Session 5: Logistic regression Part 2",
    "section": "5.3 Interpreting exponentied coefficients: Odds ratio",
    "text": "5.3 Interpreting exponentied coefficients: Odds ratio\nIn the previous session, we fitted a logistic regression model and interpreted the coefficients on the log-odds scale. However, the log of the odds is not very intuitive or it is hard to communicate for wider audiences. We can exponentiate the coefficients to obtain the odds ratio. If the logit model is:\n\\[\n\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 X\n\\]\nthen, exponentiating both sides gives:\n\\[\n\\frac{p}{1-p} = e^{\\beta_0 + \\beta_1 X} = e^{\\beta_0} \\cdot e^{\\beta_1 X}\n\\]\nThe exponentiation reverses the logarithm, transforming coefficients from the log-odds scale back to the odds ratio scale. Specifically, \\(e^{\\beta_1}\\) represents the odds ratio (OR):\n\\[\n\\text{OR} = e^{\\beta_1}\n\\]\nThus, for a one-unit increase in \\(X\\), the odds of the outcome are multiplied by \\(e^{\\beta_1}\\). Specifically:\n\nIf \\(\\beta_1 &gt; 0\\), then \\(e^{\\beta_1} &gt; 1\\), meaning the odds increase\nIf \\(\\beta_1 &lt; 0\\), then \\(e^{\\beta_1} &lt; 1\\), meaning the odds decrease\nIf \\(\\beta_1 = 0\\), then \\(e^{\\beta_1} = 1\\), meaning the odds remain unchanged\n\nFor example, if \\(\\beta_1 = 0.5\\), then \\(e^{0.5} \\approx 1.65\\), meaning the odds increase by 65% for each one-unit increase in \\(X\\).\n\n5.3.1 Odds ratio in our example\nLet’s exponentiate the results of our logistic regression model!\n\nlogit_model1 %&gt;% \n  tbl_regression(exponentiate = TRUE) %&gt;% \n  add_significance_stars(hide_ci = FALSE, hide_p = FALSE) %&gt;% \n  add_glance_table()\n\n\n\n\n  \n    \n      Characteristic\n      OR1\n      SE\n      95% CI\n      p-value\n    \n  \n  \n    free_parking\n\n\n\n\n        No\n—\n—\n—\n\n        Yes\n0.47***\n0.148\n0.35, 0.63\n&lt;0.001\n    avg_distance\n0.79***\n0.019\n0.76, 0.82\n&lt;0.001\n    gender\n\n\n\n\n        Boy/Man (cisgender or transgender)\n—\n—\n—\n\n        Girl/Woman (cisgender or transgender)\n0.60***\n0.132\n0.46, 0.77\n&lt;0.001\n        Non-binary/Something else fits better\n0.75\n0.393\n0.34, 1.60\n0.5\n        Prefer not to answer\n0.54\n0.335\n0.27, 1.02\n0.064\n    higher_education\n\n\n\n\n        No\n—\n—\n—\n\n        Yes\n1.88***\n0.175\n1.34, 2.67\n&lt;0.001\n    children\n\n\n\n\n        No\n—\n—\n—\n\n        Yes\n0.52***\n0.175\n0.36, 0.72\n&lt;0.001\n    hhincome_broad\n\n\n\n\n        Under $25,000\n—\n—\n—\n\n        $25,000-$49,999\n0.53\n0.349\n0.27, 1.06\n0.072\n        $50,000-$74,999\n0.48*\n0.353\n0.24, 0.95\n0.036\n        $75,000-$99,999\n0.49*\n0.354\n0.24, 0.98\n0.045\n        $100,000-$199,999\n0.43*\n0.332\n0.22, 0.83\n0.012\n        $200,000 or more\n0.55\n0.345\n0.28, 1.09\n0.087\n    Null deviance\n1,939\n\n\n\n    Null df\n1,680\n\n\n\n    Log-likelihood\n-761\n\n\n\n    AIC\n1,548\n\n\n\n    BIC\n1,619\n\n\n\n    Deviance\n1,522\n\n\n\n    Residual df\n1,668\n\n\n\n    No. Obs.\n1,681\n\n\n\n  \n  \n    \n      Abbreviations: CI = Confidence Interval, OR = Odds Ratio, SE = Standard Error\n    \n  \n  \n    \n      1 *p&lt;0.05; **p&lt;0.01; ***p&lt;0.001\n    \n  \n\n\n\n\nBefore anything, scan and determine which variables are significant. The confidence interval for an odds ratio reflects the range of plausible values for the true population effect. This is found in the ‘CI’ in the results table. If it the CI includes 1, it indicates that the estimated association is unlikely, connecting to statistical significance, e.g. p-value. Is there anything unexpected or looking unusual?\nWhen it comes to the the size of coefficients, the interpretations change compared to the log of the odds. For example,\n\nthe odds ratio of using active transport modes are about 50% lower for people with free parking at work compared to those without free parking.\n\nNote that we make interpretations in relation to 1. Specifically, a value lower than one implies a decrease in the odds of using active transport modes. Generally we use the following formula to express the changes in percent terms (1 - coefficient) * 100.\nFor continuous variables the interpretation is as follows:\n\nEvery additional mile to work is associated with a 20% decrease in the odds ratio for using active transport modes.\n\nNote that we are talking either about the log of the odds or the odds ratio, not the probability.\n\n\n\n\n\n\nReflection\n\n\n\nCan you provide the interpretation for rest of the coefficients, including significance and size?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5: Logistic regression Part 2</span>"
    ]
  },
  {
    "objectID": "session-5.html#model-checks",
    "href": "session-5.html#model-checks",
    "title": "5  Session 5: Logistic regression Part 2",
    "section": "5.4 Model checks",
    "text": "5.4 Model checks\n\n5.4.1 Goodness-of-fit\nIn logistic regression, the Chi-square test is used to determine whether the model with predictors fits significantly better than a model with no predictors (the null model).\n\nperformance_pcp(logit_model1)\n\n# Percentage of Correct Predictions from Logistic Regression Model\n\n  Full model: 70.52% [68.34% - 72.70%]\n  Null model: 61.18% [58.85% - 63.51%]\n\n# Likelihood-Ratio-Test\n\n  Chi-squared: 416.763\n  df:  12.000\n  p-value:   0.000\n\n\nIf the p-value is lower than 0.05, the null hypothesis is therefore rejected in favour of the alternate hypothesis that the model is better than the baseline (or null) at active travel. What does the result tell?\nFrom the output, you can also check the percentage of correct predictions, which shows how often the model’s predicted outcomes match the actual outcomes.\nWe can go further. The following function provides a the McFadden pseudo-R-squared measure.\n\nr2_mcfadden(logit_model1)\n\n# R2 for Generalized Linear Regression\n       R2: 0.215\n  adj. R2: 0.214\n\n\nThis has an analogous interpretation to adjusted R-squared in linear regression (from 0 to 1). It gives a sense of model improvement or predictive power, but it does not represent the proportion of variance explained like the R-squared in linear regression. It should be taken with care, as it is just a relative improvement (in log-likelihood) compared to the null model.\n\n\n5.4.2 Assumption checks\nThere are generally three assumptions for the logit model:\n\nindependence of observations;\nlinearity; and\nno perfect multicollinearity.\n\nWe can check for collinearity using the variance of inflation factor (VIF).\n\ncheck_collinearity(logit_model1)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n     free_parking 1.02 [1.00, 1.32]         1.01      0.98     [0.76, 1.00]\n     avg_distance 1.03 [1.00, 1.17]         1.01      0.97     [0.85, 1.00]\n           gender 1.07 [1.03, 1.15]         1.03      0.93     [0.87, 0.97]\n higher_education 1.10 [1.06, 1.18]         1.05      0.91     [0.85, 0.94]\n         children 1.07 [1.03, 1.15]         1.03      0.94     [0.87, 0.97]\n   hhincome_broad 1.24 [1.18, 1.32]         1.11      0.81     [0.76, 0.85]\n\n\nWe can check linearity examining the relationship between each continuous predictor and log-odds of the predicted probabilities. In our model, we only have one continuous variable. We can check the linearity of our model as following:\n\n# Source Harrys J.K. (2019)\n\n# Compute a variable of the log-odds of the predicted values\nlogit_pred &lt;- log(logit_model1$fitted.values / (1- logit_model1$fitted.values))\n\n# Create a small data frame with the log-odds and the distance predictor\nlinearity_data1 &lt;- \n  data.frame(\n    logit_pred, \n    avg_distance = logit_model1$model$avg_distance\n)\n\n# Plot\nlinearity_data1 %&gt;% \n  ggplot(aes(avg_distance, logit_pred)) +\n  geom_point(alpha = 0.5) +\n  geom_smooth(method = 'loess', aes(col = 'Loess curve'), se = FALSE) +\n  geom_smooth(method = 'lm', aes(col = 'Linear fit \\n(Ref.)'), se = FALSE) +\n  labs(\n    x = \"Avg. distance to work (in miles)\",\n    y= \"Log-odds of active travel \\npredicted probability\"\n  )\n\n\n\n\n\n\n\n\nYou should check whether the predictions are equally accurate along the range of values of the predictor.\nIndependence is related to the structure and collection methods of data. To what extent do your model and data meet the independence assumption?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5: Logistic regression Part 2</span>"
    ]
  },
  {
    "objectID": "session-5.html#individual-activities",
    "href": "session-5.html#individual-activities",
    "title": "5  Session 5: Logistic regression Part 2",
    "section": "5.5 Individual activities",
    "text": "5.5 Individual activities\nRe-run a similar analysis, but focus on public transport use (labelled as ‘transit’ in the data). Specifically, the dependent variable will be whether people used public transport in at least one of their trips for a purpose of your choice other than work.\nIn this analysis, the key predictor will be employers’ subsidies for public transport, such as free passes or fares. Hint: this information is available in the ‘commute_subsidy_1’ variable of the ‘persons’ table. You can keep similar control variables and optionally check how parking regulations or policies are related to the use of public transport.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5: Logistic regression Part 2</span>"
    ]
  },
  {
    "objectID": "session-5.html#references-and-further-reading",
    "href": "session-5.html#references-and-further-reading",
    "title": "5  Session 5: Logistic regression Part 2",
    "section": "5.6 References and further reading",
    "text": "5.6 References and further reading\nHarrys J.K. 2019, Statistics With R: Solving Problems Using Real-World Data. SAGE Publications (p. 651).",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5: Logistic regression Part 2</span>"
    ]
  },
  {
    "objectID": "session-6.html",
    "href": "session-6.html",
    "title": "6  Session 6: Multinomial model",
    "section": "",
    "text": "6.1 Introduction\nMultinomial regression is useful when the dependent variable is categorical and has more than two categories. This week, we’ll build on the logistic regression example from last week, where we explored mode choice for work trips and how free parking influences that decision. This time, we’ll break down mode choice at the person level, with three possible outcomes: driving, active travel, or transit.\nWe will continue to work with the Puget Sound Household Travel Survey.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6: Multinomial model</span>"
    ]
  },
  {
    "objectID": "session-6.html#preliminaries",
    "href": "session-6.html#preliminaries",
    "title": "6  Session 6: Multinomial model",
    "section": "6.2 Preliminaries",
    "text": "6.2 Preliminaries\nFor today’s session we will need the following packages. Make sure that you have these packages installed in your machine.\n\n# Packages \nlibrary(tidyverse) # For data manipulation\nlibrary(gtsummary) # Descriptive statistics\nlibrary(nnet) # For multinomial regression\nlibrary(caret) # To check the accuracy of the model\nlibrary(performance) # For model fit measures\n\nAs before, we read the data in to the R session. Again, we will obtain information from trips, persons, and households tables.\n\n# Load data\ntrips &lt;- read_csv('data/Trips.csv')\npersons &lt;- read_csv('data/Persons.csv')\nhouseholds &lt;- read_csv('data/Households.csv')\n\nWe limit the data for the year 2023 only.\n\n# Filter data for the year 2023 only\nhouseholds &lt;- households %&gt;% filter(survey_year == 2023)\ntrips &lt;- trips %&gt;% filter(survey_year == 2023)\npersons &lt;- persons %&gt;% filter(survey_year == 2023)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6: Multinomial model</span>"
    ]
  },
  {
    "objectID": "session-6.html#dependent-variable-main-mode-choice-to-work-per-person",
    "href": "session-6.html#dependent-variable-main-mode-choice-to-work-per-person",
    "title": "6  Session 6: Multinomial model",
    "section": "6.3 Dependent variable: Main mode choice to work per person",
    "text": "6.3 Dependent variable: Main mode choice to work per person\nIn this example, we focus on trips to work only. So, we keep trips which its destination is ‘Work’ only.\n\ntrips_work &lt;- trips %&gt;% \n  filter('Work' == dest_purpose_cat)\n\nNote that in the outcomes in a multinomial model must be mutually exclusive, which means that each individual can only belong to one outcome category at a time. In this example, we identify only one ‘main’ mode of transport for each person when travelling to work.\nHere, the main mode is defined as the one used most frequently. If a person reported using more than one mode the same number of times, we’ll select the one that covered the greatest distance. To start, we’ll summarise the frequency and distance reported for each person and each mode.\n\nmode_summary &lt;- trips_work %&gt;% \n  group_by(person_id, mode_class) %&gt;% \n  summarise(\n    mode_frequency = n(),\n    mode_distance = sum(distance_miles, na.rm = TRUE),\n  ) \n\nAnd, we have a quick look to the result:\n\nmode_summary\n\n# A tibble: 2,137 × 4\n# Groups:   person_id [1,821]\n    person_id mode_class mode_frequency mode_distance\n        &lt;dbl&gt; &lt;chr&gt;               &lt;int&gt;         &lt;dbl&gt;\n 1 2300017304 Drive SOV               4         16.2 \n 2 2300021301 Bike                    1          1.36\n 3 2300074501 Transit                 5         25.1 \n 4 2300082802 Transit                 1          2.07\n 5 2300116701 Drive SOV               1         25.2 \n 6 2300124701 Drive SOV               1          3.90\n 7 2300124702 Bike                    1          2.35\n 8 2300124702 Drive SOV               2          5.15\n 9 2300132201 Drive SOV               3         11.2 \n10 2300149601 Transit                 1          9.64\n# ℹ 2,127 more rows\n\n\n\n\n\n\n\n\nReflection\n\n\n\nLooking at the variables available and the summary that you just have created. Think how you can identify the ‘main’ mode before coding it.\n\n\nNow, implement the code which select the most frequently used mode for each person in the data. If there’s a tie, we’ll choose the mode that covered the most miles.\n\n# Define only one 'main' mode per person based on frequency and distance\nmode_main &lt;- mode_summary %&gt;% \n  group_by(person_id) %&gt;% \n  # Keep the most frequent modes per individual\n  slice_max(order_by = mode_frequency, with_ties = TRUE) %&gt;% \n  # If tied, pick the one with more miles\n  slice_max(order_by = mode_distance, with_ties = FALSE)  %&gt;% \n  ungroup()\n\nFor simplicity in our model, we will group mode into three broader classes, namely: Drive, Transit, and Active. For this exercise, we drop all other classes.\n\n# reclassify\nmode_main &lt;- mode_main %&gt;% \n  mutate(\n    mode_main = case_when(\n      grepl('Drive', mode_class) ~ 'Drive',\n      grepl('Transit', mode_class) ~ 'Transit',\n      grepl('Bike|Walk', mode_class) ~ 'Active',\n      # All other classes are ignored using NA\n      TRUE ~ NA\n    )\n  )\n\n# Keep only relevant column, i.e. main mode per individual\nmode_main &lt;- mode_main %&gt;% \n  select(person_id, mode_main)\n\nHow is the distribution of the main mode looking?\n\ncount(mode_main, mode_main)\n\n# A tibble: 4 × 2\n  mode_main     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Active      336\n2 Drive      1190\n3 Transit     258\n4 &lt;NA&gt;         37\n\n\n\n\n\n\n\n\nReflection\n\n\n\nWould it be possible to aggregate modal choice at a different unit of observation? Are there implications?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6: Multinomial model</span>"
    ]
  },
  {
    "objectID": "session-6.html#independent-variables",
    "href": "session-6.html#independent-variables",
    "title": "6  Session 6: Multinomial model",
    "section": "6.4 Independent variables",
    "text": "6.4 Independent variables\nIn addition to the dependent variable (mode choice in three categories), we need to define the independent variables that we think are related to mode choice to work. For this, we include information from trips, individual, and households.\nWe summarise trip information to work at the person level.\n\ntrips_distance &lt;- trips_work %&gt;% \n  group_by(person_id) %&gt;% \n  summarise(\n    total_miles = sum(distance_miles, na.rm = TRUE),\n    num_days_trips = max(daynum),\n    avg_distance = total_miles / num_days_trips\n  ) %&gt;% \n  filter(num_days_trips &gt; 0 & avg_distance &lt; 100)\n\nLater, we supplement the dependent variable information with trips, persons, and households.\n\n# Join the tables together\npersons_main &lt;- mode_main %&gt;% \n  left_join(trips_distance, by = 'person_id') %&gt;% \n  left_join(persons, by = 'person_id') %&gt;%\n  left_join(households, by = 'household_id')\n\n\n\n\n\n\n\nReflection\n\n\n\nHave a look to the number of observations in persons_main compared to persons. Why do we see this difference?\n\n\nAs in our prior lab, we format and label the variables that we will use:\n\n# Write the name and appropriate order of the levels for hhincome_broad\nincome_labs &lt;- c(\n  \"Under $25,000\", \n  \"$25,000-$49,999\", \n  \"$50,000-$74,999\", \n  \"$75,000-$99,999\", \n  \"$100,000-$199,999\", \n  \"$200,000 or more\"\n)\n\n# Categories for higher education (explicit definition)\nhe_levs &lt;- c(\n  \"Bachelor degree\",\n  \"Graduate/post-graduate degree\",\n  \"Associates degree\"\n)\n\n# Format and create appropriate variables\npersons_main &lt;- persons_main %&gt;% \n  mutate(\n    # Whether person has free parking at work\n    free_parking = ifelse(commute_subsidy_3 == 'Selected', 'Yes', 'No'),\n    # Simplify education, e.g. graduate and bachelor vs all other\n    higher_education = ifelse(education %in% he_levs, 'Yes', 'No'),\n    children = ifelse(numchildren == '0 children', 'No', 'Yes'),\n    hhincome_broad = factor(hhincome_broad, levels = income_labs)\n)\n\nAnd, keep only the variables of interest for now.\n\n# First, select variables of interest\npersons_main &lt;- persons_main %&gt;% \n  select(\n    mode_main, \n    free_parking,\n    avg_distance,\n    gender,\n    higher_education,\n    children,\n    hhincome_broad\n  )\n\n# We also remove incomplete observations\npersons_main &lt;- persons_main %&gt;% \n  drop_na()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6: Multinomial model</span>"
    ]
  },
  {
    "objectID": "session-6.html#measures-of-fit",
    "href": "session-6.html#measures-of-fit",
    "title": "6  Session 6: Multinomial model",
    "section": "9.1 Measures of fit",
    "text": "9.1 Measures of fit\nWe can compute a pseudo-r-squared value and other measures too:\n\n# Measures of fit\nmodel_performance(multinomial_model1)\n\nCan't calculate log-loss.\n\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n2289.773 | 2290.105 | 2376.377 | 0.217 |     0.216 | 0.360 | 1.173\n\n\nAs with logistic regression, we can use the Akaike information criterion (AIC). Lower AIC values indicate a better-fitting model, i.e., a model that haves a good balance between goodness of fit and complexity.\nFrom the table the column ‘R2 (adj.)’ corresponds to McFadden’s pseudo R-squared values. This value ranges from 0 to 1. Higher values indicate a better-fitting model.\nAdditionally, we can look at the predictions. For instance, the model predictions look as following:\n\n# Model predictions\nmultinomial_model1$fitted.values[1:6, ]\n\n      Drive    Active   Transit\n1 0.6800683 0.2026791 0.1172526\n2 0.2445990 0.5802809 0.1751201\n3 0.4327570 0.2992778 0.2679652\n4 0.2910892 0.5079115 0.2009993\n5 0.6078908 0.2817081 0.1104011\n6 0.5549170 0.3047215 0.1403615\n\npredict(multinomial_model1)[1:6]\n\n[1] Drive  Active Drive  Active Drive  Drive \nLevels: Drive Active Transit\n\n\nWe can check these predictions against observed data using a confusion matrix, and related measures:\n\n# Correctly predicted classes\npredicted_classes &lt;- predict(multinomial_model1)\nconfusionMatrix(predicted_classes, persons_main$mode_main)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Drive Active Transit\n   Drive    1010    127     195\n   Active     88    191      45\n   Transit     0      0       1\n\nOverall Statistics\n                                          \n               Accuracy : 0.7254          \n                 95% CI : (0.7032, 0.7468)\n    No Information Rate : 0.6626          \n    P-Value [Acc &gt; NIR] : 2.277e-08       \n                                          \n                  Kappa : 0.361           \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n\nStatistics by Class:\n\n                     Class: Drive Class: Active Class: Transit\nSensitivity                0.9199        0.6006      0.0041494\nSpecificity                0.4240        0.9007      1.0000000\nPos Pred Value             0.7583        0.5895      1.0000000\nNeg Pred Value             0.7292        0.9047      0.8550725\nPrevalence                 0.6626        0.1919      0.1454436\nDetection Rate             0.6095        0.1153      0.0006035\nDetection Prevalence       0.8039        0.1955      0.0006035\nBalanced Accuracy          0.6719        0.7507      0.5020747\n\n\nThe diagonal of the matrix indicates the number of correct predictions. The sensitivity indicates the percentage of correct predictions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6: Multinomial model</span>"
    ]
  },
  {
    "objectID": "session-7.html",
    "href": "session-7.html",
    "title": "7  Session 7: Poisson regression",
    "section": "",
    "text": "8 Introduction\nWe will now explore count models, which are relevant in transport applications for modelling variables such as the number of trips, number of vehicles, or number of collisions.\nThe Poisson model is well suited to discrete numeric outcomes ranging from 0 to infinity. Unlike linear regression, Poisson models do not allow negative values. Today, we will examine the number of active travel trips at the household level.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7: Poisson regression</span>"
    ]
  },
  {
    "objectID": "session-7.html#preliminaries",
    "href": "session-7.html#preliminaries",
    "title": "7  Session 7: Poisson regression",
    "section": "8.1 Preliminaries",
    "text": "8.1 Preliminaries\nFor today’s session we will need the following packages.\n\n# Packages \nlibrary(tidyverse) # For data manipulation\nlibrary(gtsummary) # Descriptive statistics\nlibrary(performance) # For model fit measures\n\nThis time, we will need information from trips and households tables only.\n\n# Load data\ntrips &lt;- read_csv('data/Trips.csv')\nhouseholds &lt;- read_csv('data/Households.csv')\n\nWe limit the data for the year 2023 only.\n\n# Filter data for the year 2023 only\nhouseholds &lt;- households %&gt;% filter(survey_year == 2023)\ntrips &lt;- trips %&gt;% filter(survey_year == 2023)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7: Poisson regression</span>"
    ]
  },
  {
    "objectID": "session-7.html#dependent-variable-number-of-active-travel-trips",
    "href": "session-7.html#dependent-variable-number-of-active-travel-trips",
    "title": "7  Session 7: Poisson regression",
    "section": "8.2 Dependent variable: Number of active travel trips",
    "text": "8.2 Dependent variable: Number of active travel trips\nWe will consider trips that were completed using active travel modes, regardless of the purpose.\n\nactive_trips &lt;- trips %&gt;%\n  filter(mode_class == 'Bike' | mode_class == 'Walk')\n\nNext, we count the number of active travel trip for each household.\n\n# Summarise trips by household\ntrips_summary &lt;- active_trips %&gt;% \n  group_by(household_id) %&gt;% \n  summarise(active_trips = n())\n\nHow many households report active travel trips?\nNote that the above does not include households with 0 active mode trips. So, this analysis focuses on the intensity of active travel use e.g. household who have reported at least 1 active travel trip.\nLet’s look at the distribution, and variace.\n\nsummary(trips_summary$active_trips)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   4.000   8.584  10.000  98.000 \n\nvar(trips_summary$active_trips)\n\n[1] 128.6674\n\n\nDoes the variance approximate the mean?\nRemember that this is a key assumption in Poisson regression.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7: Poisson regression</span>"
    ]
  },
  {
    "objectID": "session-7.html#independent-variables",
    "href": "session-7.html#independent-variables",
    "title": "7  Session 7: Poisson regression",
    "section": "8.3 Independent variables",
    "text": "8.3 Independent variables\nIn addition to the dependent variable (trip count), we need to define the independent variables that we think are related to mode choice to work. For this, we include information from households. So, we supplement the dependent variable information with trips, persons, and households.\n\n# Join the tables together\nhouseholds_main &lt;- trips_summary %&gt;% \n  left_join(households, by = 'household_id')\n\nHere, order matters, e.g. we join the number of active trips to all households. So, we exclude all of the households which did not reported active travel trips.\nAs in our prior lab, we format and label the variables that we will use in the analysis:\n\n# HH income labs\nincome_labs &lt;- c(\n  'Under $25,000', '$25,000-$49,999', '$50,000-$74,999', \n  '$75,000-$99,999', '$100,000-$199,999', '$200,000 or more'\n)\n\n# Format and create appropriate variables\nhouseholds_main &lt;- households_main &lt;- households_main %&gt;% \n  mutate(\n    children = ifelse(numchildren == '0 children', 'No', 'Yes'),\n    hhincome_broad = factor(hhincome_broad, levels = income_labs),\n    hhsize_int = as.numeric(str_extract(hhsize, \"\\\\d+\")),\n    vehicle_broad = case_when(\n      vehicle_count == '0 (no vehicles)' ~  '0 (no vehicles)',\n      vehicle_count == '1 vehicle' ~  '1 vehicle',\n      TRUE ~ '2 or more'\n    ),\n  )\n\nAn we keep only the variables of interest for now.\n\n# First, select variables of interest\nhouseholds_main &lt;- households_main %&gt;% \n  select(\n    active_trips,\n    children, \n    vehicle_broad,\n    hhincome_broad,\n    hhsize_int,\n    numdayscomplete,\n    vehicle_broad\n  )\n\n# We also remove incomplete observations\nhouseholds_main &lt;- households_main %&gt;% \n  drop_na()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7: Poisson regression</span>"
    ]
  },
  {
    "objectID": "session-7.html#model-assumption-checks",
    "href": "session-7.html#model-assumption-checks",
    "title": "7  Session 7: Poisson regression",
    "section": "10.1 Model assumption checks",
    "text": "10.1 Model assumption checks\nOne of the key assumptions of the Poisson model is that the mean is equal to the variance. Dispersion can occur when the data is more variable than the model assumes. This can yield to estimation problems, such as underestimated errors (e.g. we can conclude something is significant when it is not), or poor model fit.\nWe can easily test for over dispersion with the perfomance:: package as following:\n\ncheck_overdispersion(poisson_m1)\n\n# Overdispersion test\n\n       dispersion ratio =     8.978\n  Pearson's Chi-Squared = 13242.418\n                p-value =   &lt; 0.001\n\n\nFrom this test, the dispersion ratio should be close to 1. From the results we see that this is much larger, and the p-value is lower than 0.05. Thus, we can could that overdispersion is present.\nA common alternative, is to use a negative binomial model. This is also useful to model counts and does not assume that the variance is roughly similar to the mean of our count. Thus, this is appropriate when overdispersion is high. For this, we will require the glm.nb() function of the MASS:: package.\n\nlibrary(MASS)\n\n# Fit the negative binomial model\nnegbin_m2 &lt;- glm.nb(\n  formula = active_trips ~ children + vehicle_broad + hhsize_int + numdayscomplete, \n  data = households_main\n)\n\n# An we print the summary of the model\nnegbin_m2 %&gt;% \n  tbl_regression(exponentiate = TRUE) %&gt;% \n  add_glance_table()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nIRR\n95% CI\np-value\n\n\n\n\nchildren\n\n\n\n\n\n\n\n\n    No\n—\n—\n\n\n\n\n    Yes\n2.57\n2.16, 3.05\n&lt;0.001\n\n\nvehicle_broad\n\n\n\n\n\n\n\n\n    0 (no vehicles)\n—\n—\n\n\n\n\n    1 vehicle\n0.95\n0.85, 1.06\n0.4\n\n\n    2 or more\n0.73\n0.63, 0.83\n&lt;0.001\n\n\nhhsize_int\n1.19\n1.12, 1.28\n&lt;0.001\n\n\nnumdayscomplete\n\n\n\n\n\n\n\n\n    1\n—\n—\n\n\n\n\n    2\n2.03\n1.02, 4.52\n0.060\n\n\n    3\n4.12\n2.79, 6.34\n&lt;0.001\n\n\n    4\n5.15\n3.54, 7.79\n&lt;0.001\n\n\n    5\n4.85\n3.86, 6.17\n&lt;0.001\n\n\n    6\n4.24\n3.66, 4.93\n&lt;0.001\n\n\n    7\n4.13\n3.70, 4.60\n&lt;0.001\n\n\nNull deviance\n2,366\n\n\n\n\n\n\nNull df\n1,485\n\n\n\n\n\n\nLog-likelihood\n-4,418\n\n\n\n\n\n\nAIC\n8,860\n\n\n\n\n\n\nBIC\n8,924\n\n\n\n\n\n\nDeviance\n1,451\n\n\n\n\n\n\nResidual df\n1,475\n\n\n\n\n\n\nNo. Obs.\n1,486\n\n\n\n\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\nCan you identify key differences compared to the Poisson model? The interpretation of coefficients is very similar.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7: Poisson regression</span>"
    ]
  },
  {
    "objectID": "session-6.html#descriptive-statistics",
    "href": "session-6.html#descriptive-statistics",
    "title": "6  Session 6: Multinomial model",
    "section": "6.5 Descriptive statistics",
    "text": "6.5 Descriptive statistics\nBefore moving into the modelling stage, we check the main descriptive statistics.\nWe split the data according to the main dependent variable, whether respondents used active mode to travel to work.\npersons_main %&gt;% \n  tbl_summary(\n    by = mode_main,\n    statistic = all_continuous() ~ \"{mean} ± {sd}\"\n  ) %&gt;% \n  add_overall()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOverall N = 1,6571\nActive N = 3181\nDrive N = 1,0981\nTransit N = 2411\n\n\n\n\nfree_parking\n557 (34%)\n53 (17%)\n471 (43%)\n33 (14%)\n\n\navg_distance\n8 ± 8\n2 ± 3\n9 ± 9\n6 ± 6\n\n\ngender\n\n\n\n\n\n\n\n\n\n\n    Boy/Man (cisgender or transgender)\n774 (47%)\n164 (52%)\n493 (45%)\n117 (49%)\n\n\n    Girl/Woman (cisgender or transgender)\n761 (46%)\n129 (41%)\n529 (48%)\n103 (43%)\n\n\n    Non-binary/Something else fits better\n43 (2.6%)\n12 (3.8%)\n21 (1.9%)\n10 (4.1%)\n\n\n    Prefer not to answer\n79 (4.8%)\n13 (4.1%)\n55 (5.0%)\n11 (4.6%)\n\n\nhigher_education\n1,307 (79%)\n274 (86%)\n842 (77%)\n191 (79%)\n\n\nchildren\n383 (23%)\n35 (11%)\n317 (29%)\n31 (13%)\n\n\nhhincome_broad\n\n\n\n\n\n\n\n\n\n\n    Under $25,000\n54 (3.3%)\n19 (6.0%)\n18 (1.6%)\n17 (7.1%)\n\n\n    $25,000-$49,999\n186 (11%)\n46 (14%)\n105 (9.6%)\n35 (15%)\n\n\n    $50,000-$74,999\n232 (14%)\n37 (12%)\n150 (14%)\n45 (19%)\n\n\n    $75,000-$99,999\n202 (12%)\n49 (15%)\n130 (12%)\n23 (9.5%)\n\n\n    $100,000-$199,999\n592 (36%)\n94 (30%)\n432 (39%)\n66 (27%)\n\n\n    $200,000 or more\n391 (24%)\n73 (23%)\n263 (24%)\n55 (23%)\n\n\n\n1 n (%); Mean ± SD\n\n\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\nAre there noticeable differences between respondent groups?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6: Multinomial model</span>"
    ]
  },
  {
    "objectID": "session-6.html#multinomial-model-estimation",
    "href": "session-6.html#multinomial-model-estimation",
    "title": "6  Session 6: Multinomial model",
    "section": "6.6 Multinomial model estimation",
    "text": "6.6 Multinomial model estimation\nWe will need a category of reference for outcome variable in multinomial models. By default in R, the reference is the first level/category of the outcome variable. However, we will choose ‘Drive’ for our case.\n\npersons_main &lt;- persons_main %&gt;% \n  mutate(mode_main = fct_relevel(mode_main,  'Drive'))\n\nWe can fit the multinomial model using a similar syntax to what we’ve done before. The main mode (the dependent variable) is explained by whether individuals have free parking at work, as well as other control variables related to trips and socio-demographic characteristics. Important note: the multinom() function is not part of base R, but it’s available in from the nnet:: package.\n\n# Estimate Multinomial model\nmultinomial_model1 &lt;- multinom(\n  formula = mode_main ~ free_parking + avg_distance + gender + higher_education + children, \n  data = persons_main\n)\n\n# weights:  27 (16 variable)\ninitial  value 1820.400562 \niter  10 value 1151.075327\niter  20 value 1131.837168\nfinal  value 1130.330363 \nconverged\n\n\nThe standard scale of the model output is the log of the odds (similar to logistic regression). We have already discussed some aspects of interpreting results on this scale, so we will avoid repetition and move directly to interpreting the coefficients on the odds ratio scale.\n\n6.6.1 Exponentied coefficients: Odds ratio\nAs we know, the log-odds scale is not very intuitive. Similar to what we did with logistic regression, we can exponentiate the coefficients to obtain the odds ratios, which allows us to interpret the coefficients in terms of percentage changes, holding other factors constant.\n\nmultinomial_model1 %&gt;% \n  tbl_regression(exponentiate = TRUE) %&gt;% \n  add_glance_table()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOR\n95% CI\np-value\n\n\n\n\nActive\n\n\nfree_parking\n\n\n\n\n\n\n\n\n    No\n—\n—\n\n\n\n\n    Yes\n0.33\n0.23, 0.47\n&lt;0.001\n\n\navg_distance\n0.65\n0.60, 0.69\n&lt;0.001\n\n\ngender\n\n\n\n\n\n\n\n\n    Boy/Man (cisgender or transgender)\n—\n—\n\n\n\n\n    Girl/Woman (cisgender or transgender)\n0.49\n0.36, 0.67\n&lt;0.001\n\n\n    Non-binary/Something else fits better\n1.14\n0.47, 2.80\n0.8\n\n\n    Prefer not to answer\n0.78\n0.37, 1.64\n0.5\n\n\nhigher_education\n\n\n\n\n\n\n\n\n    No\n—\n—\n\n\n\n\n    Yes\n2.00\n1.33, 2.99\n&lt;0.001\n\n\nchildren\n\n\n\n\n\n\n\n\n    No\n—\n—\n\n\n\n\n    Yes\n0.39\n0.25, 0.59\n&lt;0.001\n\n\nTransit\n\n\nfree_parking\n\n\n\n\n\n\n\n\n    No\n—\n—\n\n\n\n\n    Yes\n0.22\n0.15, 0.32\n&lt;0.001\n\n\navg_distance\n0.95\n0.93, 0.97\n&lt;0.001\n\n\ngender\n\n\n\n\n\n\n\n\n    Boy/Man (cisgender or transgender)\n—\n—\n\n\n\n\n    Girl/Woman (cisgender or transgender)\n0.67\n0.49, 0.91\n0.010\n\n\n    Non-binary/Something else fits better\n1.57\n0.69, 3.61\n0.3\n\n\n    Prefer not to answer\n0.79\n0.39, 1.60\n0.5\n\n\nhigher_education\n\n\n\n\n\n\n\n\n    No\n—\n—\n\n\n\n\n    Yes\n1.27\n0.89, 1.82\n0.2\n\n\nchildren\n\n\n\n\n\n\n\n\n    No\n—\n—\n\n\n\n\n    Yes\n0.38\n0.25, 0.57\n&lt;0.001\n\n\nNA\n\n\nedf\n16.0\n\n\n\n\n\n\nDeviance\n2,261\n\n\n\n\n\n\nAIC\n2,293\n\n\n\n\n\n\nNo. Obs.\n1,657\n\n\n\n\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\nLet’s give it a go to the key independent variable:\n\nCompared to individuals without free parking at work, the odds of using active travel are 66% lower than driving for those with free parking at their workplace. Similarly, the odds of using transit are about 80% lower than driving for people with free parking at work relative to those without it.\n\nMore generally, we observe that the likelihood of both engaging in active travel and using transit is lower than driving for individuals with free parking at work compared to those without it. Additionally, the effect is more pronounced for transit than for active travel.\nAs you can see, we can use the following formula to express the changes in percent terms (1 - coefficient) * 100. However, we can also use the direct coefficient and talk about X times changes. This is helpful when we have large coefficients. For example:\n\nThe odds of engaging in active travel, compared to driving, are twice as high for individuals with a higher education degree compared to those without one.\n\nFor continuous variables the interpretation is as follows:\n\nFor every additional average mile to work, the odds of engaging in active travel are 35% lower than driving. Likewise, the odds of riding transit are 5% lower than driving for every additional mile to work.\n\n\n\n\n\n\n\nReflection\n\n\n\nGenerally speaking, the odds of both active travel and transit use are lower relative to driving and decrease as the distance to work increases. However, this effect is more pronounced for active travel. Does this difference align with your prior expectations?\nCan you provide the interpretation for rest of the coefficients?",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6: Multinomial model</span>"
    ]
  },
  {
    "objectID": "session-6.html#model-assessment",
    "href": "session-6.html#model-assessment",
    "title": "6  Session 6: Multinomial model",
    "section": "6.7 Model assessment",
    "text": "6.7 Model assessment\n\n6.7.1 Goodness-of-fit\nAs with logistic regression, we can use the Akaike information criterion (AIC). Lower AIC values indicate a better-fitting model, i.e., a model that haves a good balance between goodness of fit and complexity.\nWe can compute a pseudo-r-squared value and other measures for mutlinomial models, too:\n\n# Measures of fit\nmodel_performance(multinomial_model1)\n\nCan't calculate log-loss.\n\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n2292.661 | 2292.992 | 2379.265 | 0.216 |     0.215 | 0.360 | 1.174\n\n\nFrom the table the column ‘R2 (adj.)’ corresponds to McFadden’s pseudo R-squared values. This value ranges from 0 to 1. Higher values indicate a better-fitting model.\nAdditionally, we can look at the predictions. For instance, the model predictions look as following:\n\n# Model predictions\nmultinomial_model1$fitted.values[1:6, ]\n\n      Drive    Active   Transit\n1 0.6841346 0.2008660 0.1149995\n2 0.2465397 0.5798820 0.1735783\n3 0.4354326 0.2992195 0.2653478\n4 0.2932488 0.5075853 0.1991659\n5 0.6122968 0.2793167 0.1083864\n6 0.5541075 0.3076808 0.1382118\n\npredict(multinomial_model1)[1:6]\n\n[1] Drive  Active Drive  Active Drive  Drive \nLevels: Drive Active Transit\n\n\nWe can check these predictions against observed data using a confusion matrix, and related measures:\n\n# Correctly predicted classes\npredicted_classes &lt;- predict(multinomial_model1)\nconfusionMatrix(predicted_classes, persons_main$mode_main)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Drive Active Transit\n   Drive    1008    125     194\n   Active     90    193      47\n   Transit     0      0       0\n\nOverall Statistics\n                                          \n               Accuracy : 0.7248          \n                 95% CI : (0.7026, 0.7462)\n    No Information Rate : 0.6626          \n    P-Value [Acc &gt; NIR] : 3.081e-08       \n                                          \n                  Kappa : 0.3616          \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n\nStatistics by Class:\n\n                     Class: Drive Class: Active Class: Transit\nSensitivity                0.9180        0.6069         0.0000\nSpecificity                0.4293        0.8977         1.0000\nPos Pred Value             0.7596        0.5848            NaN\nNeg Pred Value             0.7273        0.9058         0.8546\nPrevalence                 0.6626        0.1919         0.1454\nDetection Rate             0.6083        0.1165         0.0000\nDetection Prevalence       0.8008        0.1992         0.0000\nBalanced Accuracy          0.6737        0.7523         0.5000\n\n\nThe diagonal values in the Confusion Matrix indicate the number of correct predictions. The sensitivity in Statistics by Class indicates the percentage of correct predictions. For further information consult the function documentation typing ?confusionMatrix.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6: Multinomial model</span>"
    ]
  },
  {
    "objectID": "session-6.html#individual-activities",
    "href": "session-6.html#individual-activities",
    "title": "6  Session 6: Multinomial model",
    "section": "6.8 Individual activities",
    "text": "6.8 Individual activities\nRun a similar analysis at the person level, but this time focus on sustainable mode choice for a purpose of your choice other than work. Specifically, disaggregate active travel into ‘Walk’ and ‘Bike’, and include ‘Transit’. Exclude all other modes from the model. Examine the relationship of employer subsidies for public transport in the model, such as free passes or fares. This information can be found in the ‘persons’ table. Interpret the results of the model.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6: Multinomial model</span>"
    ]
  },
  {
    "objectID": "session-6.html#bonus-multinomial-model-assumption-checks",
    "href": "session-6.html#bonus-multinomial-model-assumption-checks",
    "title": "6  Session 6: Multinomial model",
    "section": "6.9 Bonus: Multinomial model assumption checks",
    "text": "6.9 Bonus: Multinomial model assumption checks\nGenerally, there are two main assumptions for the multinomial model (Harris, 2020):\n\nIndependence of errors, and\nindependence of irrelevant alternatives (IIA).\n\nIndependence of errors relates to the structure of the data or the way it has been collected. In this case, individuals might have dependencies at the household level.\nThe IIA assumption states that the odds of choosing between any two alternatives are unaffected by the presence of other alternatives. For example, the odds of choosing active travel over driving are assumed to be unaffected by the presence of transit. In other words, if transit were removed from the model, the relative odds between drive and active would remain the same.\nThe Hausman-McFadden test is helpful to check if the IIA assumption holds. For our example, we can run the check as demonstrated below. However, we require another package, mlogit, and the structure of the data needs to be transformed.\n\nlibrary(mlogit)\n\n# Make sure all categorical variables are factors\npersons_main &lt;- persons_main %&gt;% \n  mutate(across(is.character, as.factor)) \n\n# Convert data to mlogit format\nmlogit_data &lt;- mlogit.data(\n  persons_main, \n  choice = \"mode_main\", \n  shape = \"wide\"\n)\n\n# Refit the model using mlogit()\nmlogit_model &lt;- mlogit(\n  formula = mode_main ~  0 | avg_distance + gender + higher_education + children, \n  data = mlogit_data\n)\n\n# Refit the model using mlogit() with an alternative specification\nmlogit_model_alt &lt;- mlogit(\n  formula = mode_main ~  0 | avg_distance + gender + higher_education + children, \n  data = mlogit_data,\n  alt.subset = c('Drive', 'Active')\n)\n\n# Perform Hausman-McFadden test\nhmftest(x = mlogit_model, z = mlogit_model_alt)\n\n\n    Hausman-McFadden test\n\ndata:  mlogit_data\nchisq = 98.235, df = 7, p-value &lt; 2.2e-16\nalternative hypothesis: IIA is rejected\n\n\nThe null hypothesis assumes that IIA holds. Since it’s rejected in our results, this suggests that the IIA assumption does NOT hold in this model. The implication is that the multinomial logit model might not be appropriate because the choice probabilities are affected by irrelevant alternatives. In such a case, a nested logit model can be an alternative. However, this is out of the scope of this course. More information is available at https://github.com/friendly/nestedLogit.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6: Multinomial model</span>"
    ]
  },
  {
    "objectID": "session-6.html#references",
    "href": "session-6.html#references",
    "title": "6  Session 6: Multinomial model",
    "section": "6.10 References:",
    "text": "6.10 References:\nHarrys J.K. 2020, Statistics With R: Solving Problems Using Real-World Data. Sage Publishing.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6: Multinomial model</span>"
    ]
  },
  {
    "objectID": "session-6.html#references-and-further-reading",
    "href": "session-6.html#references-and-further-reading",
    "title": "6  Session 6: Multinomial model",
    "section": "6.10 References and further reading",
    "text": "6.10 References and further reading\n\nHarris, J.K., 2019. Statistics with R: solving problems using real-world data. SAGE Publications. Chapter 11.1-11.6\nMartin, P. (2022) Regression Models for Categorical and Count Data. The Sage Quantitative Research Kit. Sage, London. Ch. 4\nChapter 8.1: Agresti, A. (2012). Categorical Data Analysis. John Wiley & Sons. [UoG library - ebook]\nChapter 9: Heeringa, S. G., West, B. T., & Berglund, P. A. (2017). Applied Survey Data Analysis, Second Edition. Chapman and Hall/CRC. https://doi.org/10.1201/9781315153278 [UoG library - ebook]\nOzbilen, B., Akar, G., White, K., Dabelko-Schoeny, H., & Cao, Q. (2022). Analysing the travel behaviour of older adults: what are the determinants of sustainable mobility? Ageing and Society, 1–29. https://doi.org/10.1017/S0144686X22001180",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6: Multinomial model</span>"
    ]
  }
]