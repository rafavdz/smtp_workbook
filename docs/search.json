[
  {
    "objectID": "session-4.html",
    "href": "session-4.html",
    "title": "4  Session 4: Logistic regression Part 1",
    "section": "",
    "text": "4.1 Preliminaries\nWe will continue to work with the Puget Sound Household Travel Survey. If you do not have the data or project set for this, please check the preliminary instructions for Week 1.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4: Logistic regression Part 1</span>"
    ]
  },
  {
    "objectID": "session-4.html#preliminaries",
    "href": "session-4.html#preliminaries",
    "title": "4  Session 4: Logistic regression Part 1",
    "section": "",
    "text": "4.1.1 Data manipulation\nFor today’s session we will need the following packages.\n\n# Packages \nlibrary(tidyverse) # For data manipulation\nlibrary(gtsummary) # Descriptive statistics\nlibrary(performance) # Model checks\n\nYou will also need to install a helper package: install.packages(\"broom.helpers\").\nNow, we will read the data in to the R session. You will need information from trips, persons, and households tables.\n\n# Load data\nhouseholds &lt;- read_csv('data/Households.csv')\npersons &lt;- read_csv('data/Persons.csv')\ntrips &lt;- read_csv('data/Trips.csv')\n\nLet’s work with data for the year 2023 only.\n\n# Filter data for the year 2023 only\nhouseholds &lt;- households %&gt;% filter(survey_year == 2023)\ntrips &lt;- trips %&gt;% filter(survey_year == 2023)\npersons &lt;- persons %&gt;% filter(survey_year == 2023)\n\n\n\n4.1.2 Summarising data at the appropriate level\n\n\n\n\n\n\nWhat is the unit of observation?\n\n\n\nMany transport datasets have a relational (hierarchical) structure. For example, repeated measurements may be nested within individuals, individuals within household, or households within places.\nBefore analysing the data, you will need to summarise the relevant information at the appropriate level of this hierarchy—that is, for the unit of observation that matches your research question.\nAsk yourself:\n\nWhat does one row of data represent at this stage of the analysis?\nAre there multiple observations per unit?\nDo I need to aggregate, average, or otherwise summarise the data to move to a higher-level unit?\n\nTake a moment to identify the unit of observation you are working with as you go through the following sections.\n\n\nIn this activity, we focus on trips to work only.\n\ntrips_work &lt;- trips %&gt;% \n  filter('Work' == dest_purpose_cat)\n\nThe dependent variable we are interested in is whether the travellers used an active transport mode or not to travel to work. This is a binary variable. So, we will use logistic regression to identify the relationships with other variables.\nThe criterion for constructing this variable is that the person has made at least one trip to work during the reported period. This information is obtained at the trip level and then summarised at the person level, as shown below.\n\n# Summarise trip data at the person level\ntrips_summary &lt;- trips_work %&gt;% \n  group_by(person_id) %&gt;% \n  summarise(\n    # Count the number of work trips made using an active mode, e.g. bike or walk\n    n_active_trips = sum(grepl(\"Walk|Bike\", mode_class)),\n    # Create a binary variable, ie whether they made any active trips\n    active_binary = ifelse(n_active_trips &gt; 0, 1, 0),\n    \n    # Total distance travelled across all work trips\n    total_miles = sum(distance_miles, na.rm = TRUE),\n    # Number of days with observed trips\n    num_days_trips = max(daynum, na.rm = TRUE),\n    # Average distance travelled per day\n    avg_distance = total_miles / num_days_trips\n)\n\nRemember, persons can generate many trips a day and use different modes. This is why we need to summarise the trip data at the person level.\nWe keep persons who reported trips to work only, and whose distance is less than 50 miles. This can be a reasonable threshold, as we are interested in active transport modes. In practice, it is always important to justify choices, e.g. literature, expert opinion, or empirical evidence.\n\ntrips_summary &lt;- trips_summary %&gt;% \n  filter(num_days_trips &gt; 0 & avg_distance &lt; 50)\n\n\n\n\n\n\n\nReflection\n\n\n\nWhat share of respondents use an active travel mode when travelling to work?\n\n\n\n\n4.1.3 Independent variables\nIn addition to the travel behaviour information, we want to know more about the demographics and characteristics of the household. Thus, we join the trip summary and household data at the person level.\n\npersons_main &lt;- persons %&gt;% \n  left_join(trips_summary, by = 'person_id') %&gt;% \n  left_join(households, by = 'household_id')\n\nNext, we format the independent variables and simplify some of these.\nWe will also create the key independent variable, namely: having free parking at work. We wish to examine if and how this is related to the use of active transport modes. We are also including further demographic control variables at the person and household level.\n\n# Write the name and appropriate order of the levels for hhincome_broad\nincome_labs &lt;- c(\n  \"Under $25,000\", \n  \"$25,000-$49,999\", \n  \"$50,000-$74,999\", \n  \"$75,000-$99,999\", \n  \"$100,000-$199,999\", \n  \"$200,000 or more\"\n)\n\n# Categories for higher education (explicit definition)\nhe_levs &lt;- c(\n  \"Bachelor degree\",\n  \"Graduate/post-graduate degree\",\n  \"Associates degree\"\n)\n\n# Format and create appropriate variables\npersons_main &lt;- persons_main %&gt;% \n  mutate(\n    # Whether person has free parking at work\n    free_parking = ifelse(commute_subsidy_3 == 'Selected', 'Yes', 'No'),\n    # Simplify education, e.g. graduate and bachelor vs all other\n    higher_education = ifelse(education %in% he_levs, 'Yes', 'No'),\n    children = ifelse(numchildren == '0 children', 'No', 'Yes'),\n    hhincome_broad = factor(hhincome_broad, levels = income_labs)\n)\n\nBefore moving to the analysis, we will keep only complete observations for the variables of interest. We will remove incomplete cases—that is, persons with missing values in any of the selected columns.\n\n\n\n\n\n\nBe careful: removing incomplete cases\n\n\n\nThis step requires caution, especially when working with supplementary variables where missing values may be expected.\nFor example, the variable age of children will naturally be missing for persons without children. Treating these cases as incomplete would incorrectly remove valid observations and result in the loss of valuable information.\nBefore dropping cases, always consider why values are missing and whether they are meaningful for the unit of observation and research question.\n\n\n\n# First, select variables of interest\npersons_main &lt;- persons_main %&gt;% \n  select(\n    active_binary, \n    free_parking,\n    avg_distance,\n    gender,\n    higher_education,\n    children,\n    hhincome_broad\n  )\n\n# Then, removing incomplete observations\npersons_main &lt;- persons_main %&gt;% \n  drop_na()",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4: Logistic regression Part 1</span>"
    ]
  },
  {
    "objectID": "session-4.html#descriptive-statistics",
    "href": "session-4.html#descriptive-statistics",
    "title": "4  Session 4: Logistic regression Part 1",
    "section": "4.2 Descriptive statistics",
    "text": "4.2 Descriptive statistics\nBefore moving on to the modelling stage, always check that the descriptive statistics are plausible and consistent, i.e. do values make sense?!. This step helps you catch data issues, unexpected patterns, or coding errors early—before they affect your models. Let’s do that now.\nPrint a simple summary of the sample including all variables selected. We split the data according to the main dependent variable, whether respondents used active mode to travel to work.\npersons_main %&gt;% \n  tbl_summary(\n    by = active_binary,\n    statistic = all_continuous() ~ \"{mean} ± {sd}\"\n  ) %&gt;% \n  add_overall()\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nOverall N = 1,6811\n0 N = 1,2381\n1 N = 4431\n\n\n\n\nfree_parking\n562 (33%)\n477 (39%)\n85 (19%)\n\n\navg_distance\n7 ± 7\n9 ± 8\n3 ± 4\n\n\ngender\n\n\n\n\n\n\n\n\n    Boy/Man (cisgender or transgender)\n787 (47%)\n562 (45%)\n225 (51%)\n\n\n    Girl/Woman (cisgender or transgender)\n772 (46%)\n583 (47%)\n189 (43%)\n\n\n    Non-binary/Something else fits better\n43 (2.6%)\n29 (2.3%)\n14 (3.2%)\n\n\n    Prefer not to answer\n79 (4.7%)\n64 (5.2%)\n15 (3.4%)\n\n\nhigher_education\n1,325 (79%)\n948 (77%)\n377 (85%)\n\n\nchildren\n384 (23%)\n326 (26%)\n58 (13%)\n\n\nhhincome_broad\n\n\n\n\n\n\n\n\n    Under $25,000\n56 (3.3%)\n29 (2.3%)\n27 (6.1%)\n\n\n    $25,000-$49,999\n191 (11%)\n132 (11%)\n59 (13%)\n\n\n    $50,000-$74,999\n237 (14%)\n177 (14%)\n60 (14%)\n\n\n    $75,000-$99,999\n203 (12%)\n143 (12%)\n60 (14%)\n\n\n    $100,000-$199,999\n598 (36%)\n467 (38%)\n131 (30%)\n\n\n    $200,000 or more\n396 (24%)\n290 (23%)\n106 (24%)\n\n\n\n1 n (%); Mean ± SD\n\n\n\n\n\n\n\n\n\n\n\n\nReflection\n\n\n\n\nAre there noticeable differences between respondent groups?\nCan you make conclusions from this?\n\n\n\n\n4.2.1 Write the data\nBefore moving ahead, save the data subset with the variables that you have formatted and created. This will also be helpful for the next session.\n\nwrite_rds(persons_main,'data/persons_main.rds')",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4: Logistic regression Part 1</span>"
    ]
  },
  {
    "objectID": "session-4.html#logistic-regression",
    "href": "session-4.html#logistic-regression",
    "title": "4  Session 4: Logistic regression Part 1",
    "section": "4.3 Logistic regression",
    "text": "4.3 Logistic regression\nWe fit a logistic regression model to identify the relationships between the dependent variable (active travel) and key independent variable (having free parking at the workplace), including control variables, such as average trip distance, gender, higher education, presence of children at household, and household income.\n\nlogit_model1 &lt;- glm(\n  active_binary ~ free_parking + avg_distance + gender + higher_education + children + hhincome_broad,\n  family = \"binomial\", \n  data = persons_main\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPrint the results of the model in the log odds ratio scale.\n\nlogit_model1 %&gt;% \n  tbl_regression() %&gt;% \n  add_glance_table()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nlog(OR)\n95% CI\np-value\n\n\n\n\nfree_parking\n\n\n\n\n\n\n\n\n    No\n—\n—\n\n\n\n\n    Yes\n-0.75\n-1.0, -0.47\n&lt;0.001\n\n\navg_distance\n-0.23\n-0.27, -0.20\n&lt;0.001\n\n\ngender\n\n\n\n\n\n\n\n\n    Boy/Man (cisgender or transgender)\n—\n—\n\n\n\n\n    Girl/Woman (cisgender or transgender)\n-0.52\n-0.78, -0.26\n&lt;0.001\n\n\n    Non-binary/Something else fits better\n-0.29\n-1.1, 0.47\n0.5\n\n\n    Prefer not to answer\n-0.62\n-1.3, 0.02\n0.064\n\n\nhigher_education\n\n\n\n\n\n\n\n\n    No\n—\n—\n\n\n\n\n    Yes\n0.63\n0.30, 0.98\n&lt;0.001\n\n\nchildren\n\n\n\n\n\n\n\n\n    No\n—\n—\n\n\n\n\n    Yes\n-0.66\n-1.0, -0.33\n&lt;0.001\n\n\nhhincome_broad\n\n\n\n\n\n\n\n\n    Under $25,000\n—\n—\n\n\n\n\n    $25,000-$49,999\n-0.63\n-1.3, 0.06\n0.072\n\n\n    $50,000-$74,999\n-0.74\n-1.4, -0.05\n0.036\n\n\n    $75,000-$99,999\n-0.71\n-1.4, -0.02\n0.045\n\n\n    $100,000-$199,999\n-0.84\n-1.5, -0.19\n0.012\n\n\n    $200,000 or more\n-0.59\n-1.3, 0.09\n0.087\n\n\nNull deviance\n1,939\n\n\n\n\n\n\nNull df\n1,680\n\n\n\n\n\n\nLog-likelihood\n-761\n\n\n\n\n\n\nAIC\n1,548\n\n\n\n\n\n\nBIC\n1,619\n\n\n\n\n\n\nDeviance\n1,522\n\n\n\n\n\n\nResidual df\n1,668\n\n\n\n\n\n\nNo. Obs.\n1,681\n\n\n\n\n\n\n\nAbbreviations: CI = Confidence Interval, OR = Odds Ratio\n\n\n\n\n\n\n\n\nFor now, the interpretation of the coefficients is on the log-odds scale. For example, the coefficient for free parking at work is -0.75 and statistically significant. This means that having free parking is associated with a decrease in the log-odds of using active transport modes compared to people without free parking:\n\\[\n\\log\\left(\\frac{p}{1-p}\\right) = \\beta_0 + \\beta_1 \\cdot \\text{FreeParking} + \\dots\n\\]\nwhere \\(p\\) is the probability of using an active mode, and \\(\\beta_1 = -0.75\\).\nFor continuous variables: We expect the log-odds of using active transport modes to decrease by 0.24 for every additional mile to work. This is also a significant predictor.\n\n\n\n\n\n\nReflection\n\n\n\nCan you provide the interpretation for other coefficients?\n\n\n\n4.3.1 Final thoughts\nSo far, the results of the logistic regression are interpreted in terms of the log of the odds. We can exponentiate the coefficient to interpret it as an odds ratio, which is often more intuitive. This is what we will do in the next hands-on session, including the discussion of model fit measures and model assumption checks.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4: Logistic regression Part 1</span>"
    ]
  },
  {
    "objectID": "session-5.html",
    "href": "session-5.html",
    "title": "5  Session 5: Logistic regression Part 2",
    "section": "",
    "text": "5.1 Preliminaries\nFor today’s session we will need the following packages.\n# Packages \nlibrary(tidyverse) # For data manipulation\nlibrary(gtsummary) # Descriptive statistics\nlibrary(performance) # Model checks\nWe will continue to work with the data subset that you created in the last session, which includes information about commuting to work trips at the individual level.\npersons_main &lt;- readRDS('data/persons_main.RDS')\nLet’s take a quick look to refresh our understanding of the data’s contents and structure.\nglimpse(persons_main)\n\nRows: 1,681\nColumns: 7\n$ active_binary    &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0,…\n$ free_parking     &lt;chr&gt; \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"No\", \"No\", \"No\"…\n$ avg_distance     &lt;dbl&gt; 2.313454, 1.360803, 4.188145, 2.065438, 2.835317, 3.9…\n$ gender           &lt;chr&gt; \"Girl/Woman (cisgender or transgender)\", \"Boy/Man (ci…\n$ higher_education &lt;chr&gt; \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"Yes\", \"No\", \"Yes\"…\n$ children         &lt;chr&gt; \"Yes\", \"No\", \"No\", \"No\", \"No\", \"No\", \"Yes\", \"Yes\", \"N…\n$ hhincome_broad   &lt;fct&gt; \"$100,000-$199,999\", \"$25,000-$49,999\", \"$200,000 or …",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5: Logistic regression Part 2</span>"
    ]
  },
  {
    "objectID": "session-5.html#model-checks",
    "href": "session-5.html#model-checks",
    "title": "5  Session 5: Logistic regression Part 2",
    "section": "6.1 Model checks",
    "text": "6.1 Model checks\nThe following function provides a the McFadden pseudo-r-squared measure.\n\nr2_mcfadden(logit_model1)\n\n# R2 for Generalized Linear Regression\n       R2: 0.216\n  adj. R2: 0.215\n\n\nWe can also check the percentage of correct predictions.\n\nperformance_pcp(logit_model1)\n\n# Percentage of Correct Predictions from Logistic Regression Model\n\n  Full model: 70.54% [68.36% - 72.72%]\n  Null model: 61.18% [58.85% - 63.51%]\n\n# Likelihood-Ratio-Test\n\n  Chi-squared: 418.031\n  df:  12.000\n  p-value:   0.000\n\n\nThere are generally three assumptions for the logit model: 1. independence of observations; 2. linearity; and 3. no perfect multicollinearity.\nWe can check for collinearity using the variance of inflation factor (VIF).\n\ncheck_collinearity(logit_model1)\n\n# Check for Multicollinearity\n\nLow Correlation\n\n             Term  VIF   VIF 95% CI Increased SE Tolerance Tolerance 95% CI\n     free_parking 1.02 [1.00, 1.38]         1.01      0.99     [0.73, 1.00]\n     avg_distance 1.03 [1.00, 1.17]         1.01      0.97     [0.85, 1.00]\n           gender 1.07 [1.03, 1.15]         1.03      0.94     [0.87, 0.97]\n higher_education 1.07 [1.03, 1.15]         1.04      0.93     [0.87, 0.97]\n         children 1.06 [1.03, 1.15]         1.03      0.94     [0.87, 0.97]\n   hhincome_broad 1.21 [1.15, 1.29]         1.10      0.83     [0.78, 0.87]\n\n\nWe can check linearity examining the relationship between each continuous predictor and log-odds of the predicted probabilities. In our model, we only have one continuous variable. We can check the linearity of our model as following:\n\n# Source Harrys J.K. (2019)\n\n# Compute a variable of the log-odds of the predicted values\nlogit_pred &lt;- log(logit_model1$fitted.values / (1- logit_model1$fitted.values))\n\n# Create a small data frame with the log-odds and the distance predictor\nlinearity_data1 &lt;- \n  data.frame(\n    logit_pred, \n    avg_distance = logit_model1$model$avg_distance\n)\n\n# Plot\nlinearity_data1 %&gt;% \n  ggplot(aes(avg_distance, logit_pred)) +\n  geom_point() +\n  geom_smooth(method = 'loess', aes(col = 'Loess curve'), se = FALSE) +\n  geom_smooth(method = 'lm', aes(col = 'Linear fit'), se = FALSE) +\n  labs(\n    x = \"Avg. distance to work (in miles)\",\n    y= \"Log-odds of active travel \\npredicted probability\"\n  )\n\n\n\n\n\n\n\n\nWe should check whether the predictions are equally accurate along the range of values of the predictor.\nIndependence is related to the structure and collection methods of data. To what extend does our model and data meet the independence assumption?",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5: Logistic regression Part 2</span>"
    ]
  },
  {
    "objectID": "session-5.html#reference",
    "href": "session-5.html#reference",
    "title": "5  Session 5: Logistic regression Part 2",
    "section": "7.1 Reference",
    "text": "7.1 Reference\nHarrys J.K. 2019, Statistics With R: Solving Problems Using Real-World Data. SAGE Publications. (p. 651)",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5: Logistic regression Part 2</span>"
    ]
  },
  {
    "objectID": "session-6.html",
    "href": "session-6.html",
    "title": "6  Session 6: Multinomial model",
    "section": "",
    "text": "7 Introduction\nWe will continue to work with the Puget Sound Household Travel Survey Version 2023.4. If you do not have the data or project set for this, please check the preliminary instructions for Week 3.\nMultinomial regression is useful when the dependent variable is categorical and has more than two categories. This week, we’ll build on the logistic regression example from last week, where we explored mode choice for work trips and how free parking influences that decision. This time, we’ll break down mode choice at the person level, with three possible outcomes: driving, active travel, or transit.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6: Multinomial model</span>"
    ]
  },
  {
    "objectID": "session-6.html#preliminaries",
    "href": "session-6.html#preliminaries",
    "title": "6  Session 6: Multinomial model",
    "section": "7.1 Preliminaries",
    "text": "7.1 Preliminaries\nFor today’s session we will need the following packages.\n\n# Packages \nlibrary(tidyverse) # For data manipulation\nlibrary(gtsummary) # Descriptive statistics\nlibrary(nnet) # For multinomial regression\nlibrary(caret) # To check the accuracy of the model\nlibrary(performance) # For model fit measures\n\nAs before, we read the data in to the R session. Again, we will obtian information from trips, persons, and households tables.\n\n# Load data\ntrips &lt;- read_csv('data/Trips.csv')\npersons &lt;- read_csv('data/Persons.csv')\nhouseholds &lt;- read_csv('data/Households.csv')\n\nWe limit the data for the year 2023 only.\n\n# Filter data for the year 2023 only\nhouseholds &lt;- households %&gt;% filter(survey_year == 2023)\ntrips &lt;- trips %&gt;% filter(survey_year == 2023)\npersons &lt;- persons %&gt;% filter(survey_year == 2023)",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6: Multinomial model</span>"
    ]
  },
  {
    "objectID": "session-6.html#dependent-variable-main-mode-choice-to-work-per-person",
    "href": "session-6.html#dependent-variable-main-mode-choice-to-work-per-person",
    "title": "6  Session 6: Multinomial model",
    "section": "7.2 Dependent variable: Main mode choice to work per person",
    "text": "7.2 Dependent variable: Main mode choice to work per person\nHere, we focus on trips to work. So, we keep trips which its destination is ‘Work’ only.\n\ntrips_work &lt;- trips %&gt;% \n  filter('Work' == dest_purpose_cat)\n\nNote that in the outcomes must be mutually exclusive. This means we need to identify only one ‘main’ mode of transport for each person when travelling to work.\nFor this exercise, we’ll define the main mode as the one that is used most frequently. If a person reported using more than one mode the same number of times, we’ll select the one that covered the greatest distance. To start, we’ll summarise the frequency and distance reported for each person and each mode.\n\nmode_summary &lt;- trips_work %&gt;% \n  group_by(person_id, mode_class) %&gt;% \n  summarise(\n    mode_frequency = n(),\n    mode_distance = sum(distance_miles, na.rm = TRUE),\n  ) \n\nAnd, we have a quick look to the result:\n\nmode_summary\n\n# A tibble: 2,137 × 4\n# Groups:   person_id [1,821]\n    person_id mode_class mode_frequency mode_distance\n        &lt;dbl&gt; &lt;chr&gt;               &lt;int&gt;         &lt;dbl&gt;\n 1 2300017304 Drive SOV               4         16.2 \n 2 2300021301 Bike                    1          1.36\n 3 2300074501 Transit                 5         25.1 \n 4 2300082802 Transit                 1          2.07\n 5 2300116701 Drive SOV               1         25.2 \n 6 2300124701 Drive SOV               1          3.90\n 7 2300124702 Bike                    1          2.35\n 8 2300124702 Drive SOV               2          5.15\n 9 2300132201 Drive SOV               3         11.2 \n10 2300149601 Transit                 1          9.64\n# ℹ 2,127 more rows\n\n\nNow, we’ll select the most frequently used mode for each individual. If there’s a tie, we’ll choose the mode that covered the most miles.\n\n# Define only one 'main' mode per person based on frequency and distance\nmode_main &lt;- mode_summary %&gt;% \n  group_by(person_id) %&gt;% \n  # Keep the most frequent modes per individual\n  slice_max(order_by = mode_frequency, with_ties = TRUE) %&gt;% \n  # If tied, pick the one with more miles\n  slice_max(order_by = mode_distance, with_ties = FALSE)  %&gt;% \n  ungroup()\n\nFor simplicity in our model, we will group mode into three broader classes, namely: Drive, Transit, and Active. For this exercise, we drop all other classes.\n\n# reclassify\nmode_main &lt;- mode_main %&gt;% \n  mutate(\n    mode_main = case_when(\n      grepl('Drive', mode_class) ~ 'Drive',\n      grepl('Transit', mode_class) ~ 'Transit',\n      grepl('Bike|Walk', mode_class) ~ 'Active',\n      # All other classes are ignored using NA\n      TRUE ~ NA\n    )\n  )\n\n# Keep only relevant column, i.e. main mode per individual\nmode_main &lt;- mode_main %&gt;% \n  select(person_id, mode_main)\n\nHow is the distribution of the main mode looking?\n\ncount(mode_main, mode_main)\n\n# A tibble: 4 × 2\n  mode_main     n\n  &lt;chr&gt;     &lt;int&gt;\n1 Active      336\n2 Drive      1190\n3 Transit     258\n4 &lt;NA&gt;         37",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6: Multinomial model</span>"
    ]
  },
  {
    "objectID": "session-6.html#independent-variables",
    "href": "session-6.html#independent-variables",
    "title": "6  Session 6: Multinomial model",
    "section": "7.3 Independent variables",
    "text": "7.3 Independent variables\nIn addition to the dependent variable (mode choice in three categories), we need to define the independent variables that we think are related to mode choice to work. For this, we include information from trips, individual, and households.\nWe summarise trip information to work at the person level.\n\ntrips_distance &lt;- trips_work %&gt;% \n  group_by(person_id) %&gt;% \n  summarise(\n    total_miles = sum(distance_miles, na.rm = TRUE),\n    num_days_trips = max(daynum),\n    avg_distance = total_miles / num_days_trips\n  ) %&gt;% \n  filter(num_days_trips &gt; 0 & avg_distance &lt; 100)\n\nLater, we supplement the dependent variable information with trips, persons, and households.\n\n# Join the tables together\npersons_main &lt;- mode_main %&gt;% \n  left_join(trips_distance, by = 'person_id') %&gt;% \n  left_join(persons, by = 'person_id') %&gt;%\n  left_join(households, by = 'household_id')\n\nHave a look to the number of observations in persons_main compared to persons. Why do we see this difference?\nAs in our prior lab, we format and label the variables that we will use:\n\n# Write the name and appropriate order of the levels for hhincome_broad\nincome_labs &lt;- c(\n  'Under $25,000', '$25,000-$49,999', '$50,000-$74,999', \n  '$75,000-$99,999', '$100,000-$199,999', '$200,000 or more'\n)\n\n# Format and create appropriate variables\npersons_main &lt;- persons_main %&gt;% \n  mutate(\n    # Whether person has free parking at work\n    free_parking = ifelse(commute_subsidy_3 == 'Selected', 'Yes', 'No'),\n    # Simplify educaton, e.g. graduate and bachelor vs all other\n    higher_education = ifelse(\n      grepl('Graduate|Bachelor', education, ignore.case = TRUE), 'Yes', 'No'),\n    children = ifelse(numchildren == '0 children', 'No', 'Yes'),\n    hhincome_broad = factor(hhincome_broad, levels = income_labs)\n  )\n\nAn we keep only the variables of interest for now.\n\n# First, select variables of interest\npersons_main &lt;- persons_main %&gt;% \n  select(\n    mode_main, \n    free_parking,\n    avg_distance,\n    gender,\n    higher_education,\n    children,\n    hhincome_broad\n  )\n\n# We also remove incomplete observations\npersons_main &lt;- persons_main %&gt;% \n  drop_na()",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6: Multinomial model</span>"
    ]
  },
  {
    "objectID": "session-6.html#measures-of-fit",
    "href": "session-6.html#measures-of-fit",
    "title": "6  Session 6: Multinomial model",
    "section": "9.1 Measures of fit",
    "text": "9.1 Measures of fit\nWe can compute a pseudo-r-squared value and other measures too:\n\n# Measures of fit\nmodel_performance(multinomial_model1)\n\nCan't calculate log-loss.\n\n\n# Indices of model performance\n\nAIC      |     AICc |      BIC |    R2 | R2 (adj.) |  RMSE | Sigma\n------------------------------------------------------------------\n2289.773 | 2290.105 | 2376.377 | 0.217 |     0.216 | 0.360 | 1.173\n\n\nAs with logistic regression, we can use the Akaike information criterion (AIC). Lower AIC values indicate a better-fitting model, i.e., a model that haves a good balance between goodness of fit and complexity.\nFrom the table the column ‘R2 (adj.)’ corresponds to McFadden’s pseudo R-squared values. This value ranges from 0 to 1. Higher values indicate a better-fitting model.\nAdditionally, we can look at the predictions. For instance, the model predictions look as following:\n\n# Model predictions\nmultinomial_model1$fitted.values[1:6, ]\n\n      Drive    Active   Transit\n1 0.6800683 0.2026791 0.1172526\n2 0.2445990 0.5802809 0.1751201\n3 0.4327570 0.2992778 0.2679652\n4 0.2910892 0.5079115 0.2009993\n5 0.6078908 0.2817081 0.1104011\n6 0.5549170 0.3047215 0.1403615\n\npredict(multinomial_model1)[1:6]\n\n[1] Drive  Active Drive  Active Drive  Drive \nLevels: Drive Active Transit\n\n\nWe can check these predictions against observed data using a confusion matrix, and related measures:\n\n# Correctly predicted classes\npredicted_classes &lt;- predict(multinomial_model1)\nconfusionMatrix(predicted_classes, persons_main$mode_main)\n\nConfusion Matrix and Statistics\n\n          Reference\nPrediction Drive Active Transit\n   Drive    1010    127     195\n   Active     88    191      45\n   Transit     0      0       1\n\nOverall Statistics\n                                          \n               Accuracy : 0.7254          \n                 95% CI : (0.7032, 0.7468)\n    No Information Rate : 0.6626          \n    P-Value [Acc &gt; NIR] : 2.277e-08       \n                                          \n                  Kappa : 0.361           \n                                          \n Mcnemar's Test P-Value : &lt; 2.2e-16       \n\nStatistics by Class:\n\n                     Class: Drive Class: Active Class: Transit\nSensitivity                0.9199        0.6006      0.0041494\nSpecificity                0.4240        0.9007      1.0000000\nPos Pred Value             0.7583        0.5895      1.0000000\nNeg Pred Value             0.7292        0.9047      0.8550725\nPrevalence                 0.6626        0.1919      0.1454436\nDetection Rate             0.6095        0.1153      0.0006035\nDetection Prevalence       0.8039        0.1955      0.0006035\nBalanced Accuracy          0.6719        0.7507      0.5020747\n\n\nThe diagonal of the matrix indicates the number of correct predictions. The sensitivity indicates the percentage of correct predictions.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6: Multinomial model</span>"
    ]
  },
  {
    "objectID": "session-7.html",
    "href": "session-7.html",
    "title": "7  Session 7: Poisson regression",
    "section": "",
    "text": "8 Introduction\nWe will now explore count models, which are relevant in transport applications for modelling variables such as the number of trips, number of vehicles, or number of collisions.\nThe Poisson model is well suited to discrete numeric outcomes ranging from 0 to infinity. Unlike linear regression, Poisson models do not allow negative values. Today, we will examine the number of active travel trips at the household level.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7: Poisson regression</span>"
    ]
  },
  {
    "objectID": "session-7.html#preliminaries",
    "href": "session-7.html#preliminaries",
    "title": "7  Session 7: Poisson regression",
    "section": "8.1 Preliminaries",
    "text": "8.1 Preliminaries\nFor today’s session we will need the following packages.\n\n# Packages \nlibrary(tidyverse) # For data manipulation\nlibrary(gtsummary) # Descriptive statistics\nlibrary(performance) # For model fit measures\n\nThis time, we will need information from trips and households tables only.\n\n# Load data\ntrips &lt;- read_csv('data/Trips.csv')\nhouseholds &lt;- read_csv('data/Households.csv')\n\nWe limit the data for the year 2023 only.\n\n# Filter data for the year 2023 only\nhouseholds &lt;- households %&gt;% filter(survey_year == 2023)\ntrips &lt;- trips %&gt;% filter(survey_year == 2023)",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7: Poisson regression</span>"
    ]
  },
  {
    "objectID": "session-7.html#dependent-variable-number-of-active-travel-trips",
    "href": "session-7.html#dependent-variable-number-of-active-travel-trips",
    "title": "7  Session 7: Poisson regression",
    "section": "8.2 Dependent variable: Number of active travel trips",
    "text": "8.2 Dependent variable: Number of active travel trips\nWe will consider trips that were completed using active travel modes, regardless of the purpose.\n\nactive_trips &lt;- trips %&gt;%\n  filter(mode_class == 'Bike' | mode_class == 'Walk')\n\nNext, we count the number of active travel trip for each household.\n\n# Summarise trips by household\ntrips_summary &lt;- active_trips %&gt;% \n  group_by(household_id) %&gt;% \n  summarise(active_trips = n())\n\nHow many households report active travel trips?\nNote that the above does not include households with 0 active mode trips. So, this analysis focuses on the intensity of active travel use e.g. household who have reported at least 1 active travel trip.\nLet’s look at the distribution, and variace.\n\nsummary(trips_summary$active_trips)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  1.000   2.000   4.000   8.584  10.000  98.000 \n\nvar(trips_summary$active_trips)\n\n[1] 128.6674\n\n\nDoes the variance approximate the mean?\nRemember that this is a key assumption in Poisson regression.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7: Poisson regression</span>"
    ]
  },
  {
    "objectID": "session-7.html#independent-variables",
    "href": "session-7.html#independent-variables",
    "title": "7  Session 7: Poisson regression",
    "section": "8.3 Independent variables",
    "text": "8.3 Independent variables\nIn addition to the dependent variable (trip count), we need to define the independent variables that we think are related to mode choice to work. For this, we include information from households. So, we supplement the dependent variable information with trips, persons, and households.\n\n# Join the tables together\nhouseholds_main &lt;- trips_summary %&gt;% \n  left_join(households, by = 'household_id')\n\nHere, order matters, e.g. we join the number of active trips to all households. So, we exclude all of the households which did not reported active travel trips.\nAs in our prior lab, we format and label the variables that we will use in the analysis:\n\n# HH income labs\nincome_labs &lt;- c(\n  'Under $25,000', '$25,000-$49,999', '$50,000-$74,999', \n  '$75,000-$99,999', '$100,000-$199,999', '$200,000 or more'\n)\n\n# Format and create appropriate variables\nhouseholds_main &lt;- households_main &lt;- households_main %&gt;% \n  mutate(\n    children = ifelse(numchildren == '0 children', 'No', 'Yes'),\n    hhincome_broad = factor(hhincome_broad, levels = income_labs),\n    hhsize_int = as.numeric(str_extract(hhsize, \"\\\\d+\")),\n    vehicle_broad = case_when(\n      vehicle_count == '0 (no vehicles)' ~  '0 (no vehicles)',\n      vehicle_count == '1 vehicle' ~  '1 vehicle',\n      TRUE ~ '2 or more'\n    ),\n  )\n\nAn we keep only the variables of interest for now.\n\n# First, select variables of interest\nhouseholds_main &lt;- households_main %&gt;% \n  select(\n    active_trips,\n    children, \n    vehicle_broad,\n    hhincome_broad,\n    hhsize_int,\n    numdayscomplete,\n    vehicle_broad\n  )\n\n# We also remove incomplete observations\nhouseholds_main &lt;- households_main %&gt;% \n  drop_na()",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7: Poisson regression</span>"
    ]
  },
  {
    "objectID": "session-7.html#model-assumption-checks",
    "href": "session-7.html#model-assumption-checks",
    "title": "7  Session 7: Poisson regression",
    "section": "10.1 Model assumption checks",
    "text": "10.1 Model assumption checks\nOne of the key assumptions of the Poisson model is that the mean is equal to the variance. Dispersion can occur when the data is more variable than the model assumes. This can yield to estimation problems, such as underestimated errors (e.g. we can conclude something is significant when it is not), or poor model fit.\nWe can easily test for over dispersion with the perfomance:: package as following:\n\ncheck_overdispersion(poisson_m1)\n\n# Overdispersion test\n\n       dispersion ratio =     8.978\n  Pearson's Chi-Squared = 13242.418\n                p-value =   &lt; 0.001\n\n\nFrom this test, the dispersion ratio should be close to 1. From the results we see that this is much larger, and the p-value is lower than 0.05. Thus, we can could that overdispersion is present.\nA common alternative, is to use a negative binomial model. This is also useful to model counts and does not assume that the variance is roughly similar to the mean of our count. Thus, this is appropriate when overdispersion is high. For this, we will require the glm.nb() function of the MASS:: package.\n\nlibrary(MASS)\n\n# Fit the negative binomial model\nnegbin_m2 &lt;- glm.nb(\n  formula = active_trips ~ children + vehicle_broad + hhsize_int + numdayscomplete, \n  data = households_main\n)\n\n# An we print the summary of the model\nnegbin_m2 %&gt;% \n  tbl_regression(exponentiate = TRUE) %&gt;% \n  add_glance_table()\n\n\n\n\n\n\n\n\n\n\n\n\n\nCharacteristic\nIRR\n95% CI\np-value\n\n\n\n\nchildren\n\n\n\n\n\n\n\n\n    No\n—\n—\n\n\n\n\n    Yes\n2.57\n2.16, 3.05\n&lt;0.001\n\n\nvehicle_broad\n\n\n\n\n\n\n\n\n    0 (no vehicles)\n—\n—\n\n\n\n\n    1 vehicle\n0.95\n0.85, 1.06\n0.4\n\n\n    2 or more\n0.73\n0.63, 0.83\n&lt;0.001\n\n\nhhsize_int\n1.19\n1.12, 1.28\n&lt;0.001\n\n\nnumdayscomplete\n\n\n\n\n\n\n\n\n    1\n—\n—\n\n\n\n\n    2\n2.03\n1.02, 4.52\n0.060\n\n\n    3\n4.12\n2.79, 6.34\n&lt;0.001\n\n\n    4\n5.15\n3.54, 7.79\n&lt;0.001\n\n\n    5\n4.85\n3.86, 6.17\n&lt;0.001\n\n\n    6\n4.24\n3.66, 4.93\n&lt;0.001\n\n\n    7\n4.13\n3.70, 4.60\n&lt;0.001\n\n\nNull deviance\n2,366\n\n\n\n\n\n\nNull df\n1,485\n\n\n\n\n\n\nLog-likelihood\n-4,418\n\n\n\n\n\n\nAIC\n8,860\n\n\n\n\n\n\nBIC\n8,924\n\n\n\n\n\n\nDeviance\n1,451\n\n\n\n\n\n\nResidual df\n1,475\n\n\n\n\n\n\nNo. Obs.\n1,486\n\n\n\n\n\n\n\nAbbreviations: CI = Confidence Interval, IRR = Incidence Rate Ratio\n\n\n\n\n\n\n\n\nCan you identify key differences compared to the Poisson model? The interpretation of coefficients is very similar.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7: Poisson regression</span>"
    ]
  }
]